{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#hyper-parameters\n",
    "INPUT_LAYER_SIZE=784\n",
    "OUTPUT_LAYER_SIZE=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('./data/train.csv')\n",
    "val=pd.read_csv('./data/val.csv')\n",
    "test=pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_class =train.iloc[:,-1].copy().as_matrix()\n",
    "training_data=train.iloc[:,1:785].copy().as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validating_data_class =val.iloc[:,-1].copy().as_matrix()\n",
    "validating_data=val.iloc[:,1:785].copy().as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model=nn.MLPClassifier(activation='logistic',hidden_layer_sizes=(784,400,200,100,50,25),)\n",
    "model.fit(training_data[:100],training_data_class[:100])\n",
    "result=model.predict(training_data[:100])\n",
    "r=training_data_class[:100]-result\n",
    "print 1.0*np.count_nonzero(r)/len(validating_data_class[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_output(class_labels):\n",
    "    output=np.zeros((len(class_labels),10))\n",
    "    for i in range(len(class_labels)):\n",
    "        output[i,class_labels[i]]=1\n",
    "    return output\n",
    "def sigmoid(value):\n",
    "    return 1/(1+np.exp(-value))\n",
    "def tanh(value):\n",
    "    return np.tanh(value)\n",
    "def sigmoid_dif(value):\n",
    "    return sigmoid(value)*(1-sigmoid(value))\n",
    "def tanh_dif(value):\n",
    "    return 1-tanh(value)*tanh(value)\n",
    "def softmax(vector):\n",
    "    num= np.exp(vector)\n",
    "    return num/np.sum(num)\n",
    "def paraCopy(w,b,multiplier=1):\n",
    "    return [i*multiplier for i in w],[i*multiplier for i in b]\n",
    "def paraAdd(A,B):\n",
    "    a,b=A\n",
    "    c,d=B\n",
    "    return [i+j for i,j in zip(a,c)],[i+j for i,j in zip(b,d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shaper(lst):\n",
    "    for i in lst:\n",
    "        print 'shape -',i.shape\n",
    "def loss(list1,list2):\n",
    "    return -np.sum([j*np.log2(i) for i,j in zip(list1,list2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Model:\n",
    "    def __init__(self,lr=0.01,momentum=0,hidden_layers=(100,),activation='sigmoid',loss='sq',\n",
    "                 opt='gd',batch_size=1,anneal=False,save_dir='./temp/',expt_dir='./temp/',max_iter=50):\n",
    "        self.lr=lr\n",
    "        self.momentum=momentum\n",
    "        self.layers=(INPUT_LAYER_SIZE,)+hidden_layers+(OUTPUT_LAYER_SIZE,)\n",
    "        if activation=='sigmoid':\n",
    "            self.activation=sigmoid\n",
    "        elif activation=='tanh':\n",
    "            self.activation=tanh\n",
    "        else:\n",
    "            print 'Error : activation function not found'\n",
    "                \n",
    "        \n",
    "        self.loss=loss\n",
    "        self.opt=opt\n",
    "        self.batch_size=batch_size\n",
    "        self.anneal=anneal\n",
    "        self.save_dir=save_dir\n",
    "        self.expt_dir=expt_dir\n",
    "        self.num_layers=len(self.layers)\n",
    "        self.max_iter=max_iter\n",
    "    \n",
    "    def __forward_propagation(self):\n",
    "        h_set=[]\n",
    "        a_set=[]\n",
    "        h=self.input_data[TRAINER]\n",
    "        h_set.append(h) #experimental\n",
    "        L=self.num_layers-2 # 2 are input and output layers\n",
    "        for k in range(L): \n",
    "            a=self.biases[k]+np.matmul(self.weights[k],h)\n",
    "            h=self.activation(a)\n",
    "            a_set.append(a)\n",
    "            h_set.append(h)\n",
    "        a=self.biases[L]+np.matmul(self.weights[L],h)\n",
    "        a_set.append(a)\n",
    "        y=softmax(a)\n",
    "        #h_set.append(y) #experimental\n",
    "        #print h.shape\n",
    "            \n",
    "        return h_set,a_set,y\n",
    "    \n",
    "    def __forward_propagation_test(self,h):\n",
    "        L=self.num_layers-2 # 2 are input and output layers\n",
    "        for k in range(L): \n",
    "            a=self.biases[k]+np.matmul(self.weights[k],h)\n",
    "            h=self.activation(a)\n",
    "        a=self.biases[L]+np.matmul(self.weights[L],h)\n",
    "        y=softmax(a)\n",
    "        #h_set.append(y) #experimental\n",
    "        #print h.shape\n",
    "            \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def __back_propagation(self,h,a,y):\n",
    "        L=self.num_layers-2\n",
    "        dWeights=[]\n",
    "        dBiases=[]\n",
    "        \n",
    "        daL_loss=-(self.output_data[TRAINER]-y) # for cross-entropy loss function\n",
    "        dA_loss=daL_loss\n",
    "        for k in range(L,-1,-1):\n",
    "            #print 'k=',k\n",
    "            dW_loss=np.outer(dA_loss,h[k]).T\n",
    "            dB_loss=dA_loss\n",
    "            \n",
    "            if k!=0:\n",
    "                dH_loss=np.matmul(self.weights[k].T,dA_loss)\n",
    "            \n",
    "                dA_loss=dH_loss*sigmoid_dif(a[k-1])\n",
    "            \n",
    "            \n",
    "            dWeights=[dW_loss.T]+dWeights\n",
    "            dBiases=[dB_loss.T]+dBiases\n",
    "            #print dA_loss.shape\n",
    "            #return _,_\n",
    "            \n",
    "            \n",
    "        return dWeights,dBiases\n",
    "    \n",
    "    def __update(self,dWeights,dBiases):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i]=self.weights[i]-dWeights[i]\n",
    "            self.biases[i]=self.biases[i]-dBiases[i]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __train(self):\n",
    "        global TRAINER\n",
    "        for i in range(self.max_iter):\n",
    "            los=0\n",
    "            accuracy=0\n",
    "            if self.batch_size==1:\n",
    "                sets=1\n",
    "            else:\n",
    "                sets=len(training_data)/self.batch_size\n",
    "                \n",
    "            oldDWeights,oldDBiases=paraCopy(self.weights,self.biases,0) # for momentum\n",
    "            for j in sets:\n",
    "                for TRAINER in range(sets):\n",
    "\n",
    "                    h,a,y=self.__forward_propagation()\n",
    "\n",
    "                    dWeights,dBiases= self.__back_propagation(h,a,y)\n",
    "                    dWeights,dBiases=paraAdd(paraCopy(oldDWeights,oldDBiases,self.momentum),\n",
    "                                             paraCopy(dWeights,dBiases,self.lr))\n",
    "\n",
    "                    self.__update(dWeights,dBiases)\n",
    "\n",
    "\n",
    "                    #los+=loss(y,self.output_data[TRAINER])\n",
    "                    oldDWeights,oldDBiases=paraCopy(dWeights,dBiases,1)\n",
    "\n",
    "                    #print np.argmax(y),self.raw_class_labels[TRAINER]\n",
    "                    if np.argmax(y)==self.raw_class_labels[TRAINER]:\n",
    "\n",
    "                        accuracy+=1\n",
    "                #print 'loss=',los\n",
    "            print 'acc=',accuracy*1.0/sets\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        self.input_data=X\n",
    "        self.output_data=create_output(Y)\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        self.raw_class_labels=Y\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.random.randint(-10,10,size=(self.layers[i+1],self.layers[i])))\n",
    "            self.biases.append(np.random.randint(-10,10,size=(self.layers[i+1])))\n",
    "            \n",
    "        self.__train()\n",
    "        \n",
    "    def predict(self,X):\n",
    "        return [np.argmax(self.__forward_propagation_test(i)) for i in X]    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NN_Model(hidden_layers=(392,196,98,49,25),lr=0.01,max_iter=40,momentum=0.5)\n",
    "training_data=np.array(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mak/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc= 0.0998909090909\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-374-145c99278c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_data_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-372-d0bc37a8159b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-372-d0bc37a8159b>\u001b[0m in \u001b[0;36m__train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mTRAINER\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mdWeights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdBiases\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__back_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-372-d0bc37a8159b>\u001b[0m in \u001b[0;36m__forward_propagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# 2 are input and output layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0ma_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(training_data,training_data_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mak/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100-np.count_nonzero(model.predict(validating_data[:100])-validating_data_class[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 8, 0, 6, 5, 8, 0, 4, 7, 8, 6, 7, 0, 9, 7, 3, 5, 0, 6, 0, 7, 7, 1,\n",
       "       6, 0, 9, 2, 7, 4, 2, 1, 2, 1, 6, 3, 5, 4, 5, 9, 9, 7, 7, 5, 5, 6, 4,\n",
       "       1, 1, 3, 2, 1, 4, 2, 8, 9, 1, 8, 6, 7, 0, 9, 5, 9, 4, 6, 3, 3, 7, 9,\n",
       "       3, 3, 9, 0, 1, 4, 2, 0, 3, 0, 3, 1, 2, 7, 3, 2, 8, 8, 8, 1, 7, 1, 6,\n",
       "       8, 1, 5, 1, 0, 5, 5, 7])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validating_data_class[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
