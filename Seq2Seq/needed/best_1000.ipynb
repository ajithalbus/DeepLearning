{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO='<go>'\n",
    "STOP='<stop>'\n",
    "PAD='<pad>'\n",
    "BATCH=5\n",
    "ATTEND=False\n",
    "BEAM=True\n",
    "BEAM_WIDTH=5\n",
    "EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breaker(listofsentences,start=True):\n",
    "    '''returns a list of list of strings'''\n",
    "    if listofsentences==None:\n",
    "        return None\n",
    "    lst=[]\n",
    "    for i in listofsentences:\n",
    "        t=i.split()\n",
    "        if start:\n",
    "            t=[GO]+t+[PAD]\n",
    "        else:\n",
    "            t=[GO]+t+[STOP]\n",
    "        lst.append(t)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(data='train'):\n",
    "    COMBINED='./WeatherGov/'+data+'/'+data+'.combined'\n",
    "    SUMMARY='./WeatherGov/'+data+'/summaries.txt'\n",
    "    op_sec=None\n",
    "    with open(COMBINED) as f:\n",
    "        content = f.readlines()\n",
    "    ip_sec = [x.strip() for x in content]\n",
    "    if data!='test':\n",
    "        with open(SUMMARY) as f:\n",
    "            content = f.readlines()\n",
    "        op_sec = [x.strip() for x in content]\n",
    "        \n",
    "    return breaker(ip_sec,True),breaker(op_sec,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listofwords(data):\n",
    "    '''takes a list of sentences nd returns vocab'''\n",
    "    a=[]\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            if j not in a:\n",
    "                a.append(j)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ip_sec,training_op_sec=read('train')\n",
    "valid_ip_sec,valid_op_sec=read('dev')\n",
    "test_ip_sec,_=read('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_vocab=listofwords(training_ip_sec)\n",
    "op_vocab=listofwords(training_op_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_vocab_size,op_vocab_size=len(ip_vocab),len(op_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ip_sec_len=[len(i) for i in training_ip_sec]\n",
    "training_op_sec_len=[len(i) for i in training_op_sec]\n",
    "valid_ip_sec_len=[len(i) for i in valid_ip_sec]\n",
    "valid_op_sec_len=[len(i) for i in valid_op_sec]\n",
    "test_ip_sec_len=[len(i) for i in test_ip_sec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ip=pre.LabelEncoder()\n",
    "pre_ip.fit(ip_vocab)\n",
    "\n",
    "onehoter=np.identity(len(pre_ip.classes_))\n",
    "\n",
    "#word to int\n",
    "training_ip_sect=[pre_ip.transform(i) for i in training_ip_sec]\n",
    "valid_ip_sect=[pre_ip.transform(i) for i in valid_ip_sec]\n",
    "test_ip_sect=[pre_ip.transform(i) for i in test_ip_sec]\n",
    "\n",
    "#adding pads\n",
    "maxi=0\n",
    "for i in training_ip_sect:\n",
    "    maxi=max(maxi,len(i))\n",
    "training_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in training_ip_sect]\n",
    "valid_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in valid_ip_sect]\n",
    "test_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in test_ip_sect]\n",
    "\n",
    "#making them onehot\n",
    "training_ip_sectt=[[onehoter[i] for i in j] for j in training_ip_sect0]\n",
    "valid_ip_sectt=[[onehoter[i] for i in j] for j in valid_ip_sect0]\n",
    "test_ip_sectt=[[onehoter[i] for i in j] for j in test_ip_sect0]\n",
    "\n",
    "\n",
    "#del training_ip_sec,valid_ip_sec,test_ip_sec\n",
    "#del training_ip_sect,valid_ip_sect,test_ip_sect\n",
    "#del training_ip_sect0,valid_ip_sect0,test_ip_sect0\n",
    "\n",
    "#OP sec\n",
    "pre_op=pre.LabelEncoder()\n",
    "pre_op.fit(op_vocab)\n",
    "\n",
    "onehoter2=np.identity(len(pre_op.classes_))\n",
    "\n",
    "#word to int\n",
    "training_op_sect=[pre_op.transform(i) for i in training_op_sec]\n",
    "valid_op_sect=[pre_op.transform(i) for i in valid_op_sec]\n",
    "\n",
    "#adding pad\n",
    "maxi=0\n",
    "for i in training_op_sect:\n",
    "    maxi=max(maxi,len(i))\n",
    "training_op_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_op.transform([STOP])) for i in training_op_sect]\n",
    "valid_op_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_op.transform([STOP])) for i in valid_op_sect]\n",
    "\n",
    "\n",
    "#int to onehot\n",
    "training_op_sectt=[[onehoter2[i] for i in j] for j in training_op_sect0]\n",
    "valid_op_sectt=[[onehoter2[i] for i in j] for j in valid_op_sect0]\n",
    "\n",
    "training_op_sectt_nopad=[[onehoter2[i] for i in j] for j in training_op_sect]\n",
    "valid_op_sectt_nopad=[[onehoter2[i] for i in j] for j in valid_op_sect]\n",
    "\n",
    "\n",
    "del training_op_sec,valid_op_sec\n",
    "del training_op_sect,valid_op_sect\n",
    "#del training_op_sect0,valid_op_sect0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "maxi=0\n",
    "for i in training_op_sect0:\n",
    "    maxi=max(maxi,len(i))\n",
    "print maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using sect\n",
    "embedding_size=256\n",
    "lstm_units=512\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_seq = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "target_seq = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "source_seq_len = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "target_seq_len = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "no_start_target_seq = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "trainer = tf.placeholder(shape=(None),dtype=tf.bool)\n",
    "batch_size = tf.placeholder(shape=(None),dtype=tf.int32)\n",
    "real_target_seq_len= tf.placeholder(shape=(None,), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_encode = tf.get_variable(\n",
    "    name=\"embedding_matrix_en\",\n",
    "    shape=[ip_vocab_size, embedding_size],\n",
    "    dtype=tf.float32)\n",
    "embedding_matrix_decode = tf.get_variable(\n",
    "    name=\"embedding_matrix_de\",\n",
    "    shape=[op_vocab_size, embedding_size],\n",
    "    dtype=tf.float32)\n",
    "source_seq_embedded = tf.nn.embedding_lookup(embedding_matrix_encode, source_seq) \n",
    "decoder_input_embedded = tf.nn.embedding_lookup(embedding_matrix_decode, target_seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(encoder_outputs_fw,encoder_outputs_bw) ,(encoder_state_fw,encoder_state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "    tf.contrib.rnn.LSTMCell(lstm_units),tf.contrib.rnn.LSTMCell(lstm_units),\n",
    "    source_seq_embedded,\n",
    "    sequence_length=source_seq_len,\n",
    "    dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_states=tf.concat((encoder_outputs_fw,encoder_outputs_bw),1)\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(lstm_units*2, attention_states,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_final_state_c = tf.layers.dropout(inputs=tf.concat(\n",
    "    (encoder_state_fw.c, encoder_state_bw.c), 1),rate=0.4,training=trainer)\n",
    "\n",
    "encoder_final_state_h = tf.layers.dropout(inputs=tf.concat(\n",
    "    (encoder_state_fw.h, encoder_state_bw.h), 1),rate=0.4,training=trainer)\n",
    "\n",
    "encoder_final_state = tf.contrib.rnn.LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.layers.Dense(op_vocab_size)\n",
    "\n",
    "decoder_cell=tf.contrib.rnn.LSTMCell(lstm_units*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state=encoder_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if ATTEND:\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism,\n",
    "        attention_layer_size=lstm_units*2,alignment_history=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ATTEND:\n",
    "    decoder_initial_state = decoder_cell.zero_state(BATCH, tf.float32).clone(cell_state=decoder_initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(decoder_input_embedded,target_seq_len)\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state=decoder_initial_state,output_layer=output_layer)#,output_layer=projection_layer)\n",
    "outputs, state, seq_len = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "logits = outputs.rnn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper2 = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_matrix_decode,tf.fill([batch_size],\n",
    "                                                    np.int32(pre_op.transform([GO])[0])),\n",
    "                                                   np.int32(pre_op.transform([STOP])[0]))\n",
    "\n",
    "\n",
    "decoder2 = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper2, decoder_initial_state,output_layer=output_layer)#,output_layer=projection_layer)\n",
    "\n",
    "outputs, state, seq_len = tf.contrib.seq2seq.dynamic_decode(decoder2,maximum_iterations=maxi+10)\n",
    "\n",
    "translations_logits = outputs.rnn_output\n",
    "trs=outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##beam\n",
    "\n",
    "decoder_initial_state_beam = tf.contrib.seq2seq.tile_batch(\n",
    "    decoder_initial_state, multiplier=BEAM_WIDTH)\n",
    "\n",
    "# Define a beam-search decoder\n",
    "decoder3 = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding_matrix_decode,\n",
    "        start_tokens=tf.fill([batch_size],np.int32(pre_op.transform([GO])[0])),\n",
    "        end_token=np.int32(pre_op.transform([STOP])[0]),\n",
    "        initial_state=decoder_initial_state_beam,\n",
    "        beam_width=BEAM_WIDTH,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=0.0)\n",
    "outputs, state, seq_len = tf.contrib.seq2seq.dynamic_decode(decoder3,maximum_iterations=maxi+10)\n",
    "\n",
    "\n",
    "trs_beam=outputs.predicted_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=no_start_target_seq,logits=logits)\n",
    "\n",
    "target_weights = tf.sequence_mask(real_target_seq_len, target_seq_len[0], dtype=logits.dtype)\n",
    "\n",
    "loss=tf.reduce_sum(cross_entropy*target_weights)\n",
    "train = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxtlen=max(training_op_sec_len)\n",
    "maxvlen=max(valid_op_sec_len)\n",
    "t_newlen=[maxtlen-1 for i in range(len(training_op_sec_len))]\n",
    "v_newlen=[maxtlen-1 for i in range(len(valid_op_sec_len))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-8e2675bc5966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                 \u001b[0mno_start_target_seq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op_sect0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                                 \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                                                 })\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_losses=[]\n",
    "valid_losses=[]\n",
    "for j in range(EPOCHS):\n",
    "    training_loss=0\n",
    "    for i in range(len(training_ip_sectt)/BATCH):\n",
    "        start=i*BATCH\n",
    "        stop=(i+1)*BATCH\n",
    "        \n",
    "        _,lost=sess.run([train,loss],feed_dict={source_seq:training_ip_sect0[start:stop],\n",
    "                                                target_seq:training_op_sect0[start:stop],\n",
    "                                              source_seq_len:training_ip_sec_len[start:stop],\n",
    "                                                target_seq_len:t_newlen[start:stop],\n",
    "                                                real_target_seq_len:training_op_sec_len[start:stop],\n",
    "                                                no_start_target_seq:np.array(training_op_sect0[start:stop])[:,1:],\n",
    "                                                batch_size:BATCH,\n",
    "                                                trainer:True\n",
    "                                                })\n",
    "        \n",
    "        training_loss+=lost\n",
    "        \n",
    "    #calculate t_loss\n",
    "    training_losses.append(training_loss/len(training_ip_sectt))\n",
    "    \n",
    "    #calculate v_loss\n",
    "    validation_loss=0\n",
    "    for k in range(len(valid_ip_sectt)/BATCH):\n",
    "        start=k*BATCH\n",
    "        stop=(k+1)*BATCH\n",
    "\n",
    "        lost=sess.run(loss,feed_dict={source_seq:valid_ip_sect0[start:stop],\n",
    "                                                target_seq:valid_op_sect0[start:stop],\n",
    "                                              source_seq_len:valid_ip_sec_len[start:stop],\n",
    "                                                target_seq_len:v_newlen[start:stop],\n",
    "                                                    real_target_seq_len:valid_op_sec_len[start:stop],\n",
    "                                                    no_start_target_seq:np.array(valid_op_sect0[start:stop])[:,1:],\n",
    "                                                    batch_size:BATCH,\n",
    "                                                    trainer:False})\n",
    "        validation_loss += lost\n",
    "        \n",
    "    valid_losses.append(validation_loss/len(valid_ip_sectt))\n",
    "    print \"Epoch:%d training loss%.4f: valid loss:%.4f\"% (j,training_losses[-1],valid_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer=open('./losses_b250.txt','w+')\n",
    "for tl,vl in zip(training_losses,valid_losses):\n",
    "    filer.write(\"%f,%f\\n\" % (tl,vl))\n",
    "filer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x,y=sess.run([translations,trs],feed_dict={source_seq:valid_ip_sect0[0:BATCH],\n",
    "                                               source_seq_len:valid_ip_sec_len[0:BATCH],\n",
    "                                           trainer:False,batch_size:BATCH\n",
    "                                                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre_op.inverse_transform(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre_op.inverse_transform(valid_op_sect0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=test_ip_sect0\n",
    "data_len=test_ip_sec_len\n",
    "gen_sum=[]\n",
    "for i in range(1):#len(data)/BATCH):\n",
    "    start=i*BATCH\n",
    "    stop=(i+1)*BATCH\n",
    "    y=sess.run(trs_beam,feed_dict={source_seq:data[start:stop],\n",
    "                                               source_seq_len:data_len[start:stop],\n",
    "                                              batch_size:BATCH,\n",
    "                                                    trainer:False\n",
    "                                                })\n",
    "    for t in y:\n",
    "        gen_sum.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 329, 329,\n",
       "       329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329,\n",
       "       329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329,\n",
       "       329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329,\n",
       "       329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329,\n",
       "       329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329,\n",
       "       329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329, 329,\n",
       "       329, 329, 329, 329, 329, 329, 329, 329, 329], dtype=int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "summs=[]\n",
    "for i in gen_sum:\n",
    "    summ=''\n",
    "    for j in i:\n",
    "        \n",
    "        if j!=205:\n",
    "            summ = summ+' '+pre_op.inverse_transform(j)\n",
    "    summs.append(summ[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer=open('./model-1_test','w+')\n",
    "for item in summs:\n",
    "    filer.write(\"%s\\n\" % item)\n",
    "filer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,stop=0,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v_newlen[1000:1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=sess.run([target_seq,logits],feed_dict={source_seq:training_ip_sect0[start:stop],\n",
    "                                                target_seq:training_op_sect0[start:stop],\n",
    "                                              source_seq_len:training_ip_sec_len[start:stop],\n",
    "                                                target_seq_len:v_newlen[start:stop],\n",
    "                                                \n",
    "                                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_op.inverse_transform([x[1],np.argmax(y[1],axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_ip_sect0[1000:1020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(v_newlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(training_op_sect0)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver = tf.train.Saver()\n",
    "#saver.save(sess, './model/batch',global_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph('./model/batch-1000.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./model/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_ip=[]\n",
    "for i in range(len(pre_ip.classes_)):\n",
    "    dummy_ip.append(pre_ip.inverse_transform([i])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer=open('./pre_ip.txt','w+')\n",
    "for tl in dummy_ip:\n",
    "    filer.write(\"%s\\n\" % tl)\n",
    "    print tl\n",
    "filer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
