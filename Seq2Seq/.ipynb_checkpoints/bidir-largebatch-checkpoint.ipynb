{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO='<go>'\n",
    "STOP='<stop>'\n",
    "PAD='<pad>'\n",
    "BATCH=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breaker(listofsentences,start=True):\n",
    "    '''returns a list of list of strings'''\n",
    "    if listofsentences==None:\n",
    "        return None\n",
    "    lst=[]\n",
    "    for i in listofsentences:\n",
    "        t=i.split()\n",
    "        if start:\n",
    "            t=[GO]+t+[PAD]\n",
    "        else:\n",
    "            t=[GO]+t+[STOP]\n",
    "        lst.append(t)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(data='train'):\n",
    "    COMBINED='./WeatherGov/'+data+'/'+data+'.combined'\n",
    "    SUMMARY='./WeatherGov/'+data+'/summaries.txt'\n",
    "    op_sec=None\n",
    "    with open(COMBINED) as f:\n",
    "        content = f.readlines()\n",
    "    ip_sec = [x.strip() for x in content]\n",
    "    if data!='test':\n",
    "        with open(SUMMARY) as f:\n",
    "            content = f.readlines()\n",
    "        op_sec = [x.strip() for x in content]\n",
    "        \n",
    "    return breaker(ip_sec,True),breaker(op_sec,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listofwords(data):\n",
    "    '''takes a list of sentences nd returns vocab'''\n",
    "    a=[]\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            if j not in a:\n",
    "                a.append(j)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ip_sec,training_op_sec=read('train')\n",
    "valid_ip_sec,valid_op_sec=read('dev')\n",
    "test_ip_sec,_=read('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_vocab=listofwords(training_ip_sec)\n",
    "op_vocab=listofwords(training_op_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_vocab_size,op_vocab_size=len(ip_vocab),len(op_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ip_sec_len=[len(i) for i in training_ip_sec]\n",
    "training_op_sec_len=[len(i) for i in training_op_sec]\n",
    "valid_ip_sec_len=[len(i) for i in valid_ip_sec]\n",
    "valid_op_sec_len=[len(i) for i in valid_op_sec]\n",
    "test_ip_sec_len=[len(i) for i in test_ip_sec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ip=pre.LabelEncoder()\n",
    "pre_ip.fit(ip_vocab)\n",
    "\n",
    "onehoter=np.identity(len(pre_ip.classes_))\n",
    "\n",
    "#word to int\n",
    "training_ip_sect=[pre_ip.transform(i) for i in training_ip_sec]\n",
    "valid_ip_sect=[pre_ip.transform(i) for i in valid_ip_sec]\n",
    "test_ip_sect=[pre_ip.transform(i) for i in test_ip_sec]\n",
    "\n",
    "#adding pads\n",
    "maxi=0\n",
    "for i in training_ip_sect:\n",
    "    maxi=max(maxi,len(i))\n",
    "training_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in training_ip_sect]\n",
    "valid_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in valid_ip_sect]\n",
    "test_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in test_ip_sect]\n",
    "\n",
    "#making them onehot\n",
    "training_ip_sectt=[[onehoter[i] for i in j] for j in training_ip_sect0]\n",
    "valid_ip_sectt=[[onehoter[i] for i in j] for j in valid_ip_sect0]\n",
    "test_ip_sectt=[[onehoter[i] for i in j] for j in test_ip_sect0]\n",
    "\n",
    "\n",
    "#del training_ip_sec,valid_ip_sec,test_ip_sec\n",
    "#del training_ip_sect,valid_ip_sect,test_ip_sect\n",
    "#del training_ip_sect0,valid_ip_sect0,test_ip_sect0\n",
    "\n",
    "#OP sec\n",
    "pre_op=pre.LabelEncoder()\n",
    "pre_op.fit(op_vocab)\n",
    "\n",
    "onehoter2=np.identity(len(pre_op.classes_))\n",
    "\n",
    "#word to int\n",
    "training_op_sect=[pre_op.transform(i) for i in training_op_sec]\n",
    "valid_op_sect=[pre_op.transform(i) for i in valid_op_sec]\n",
    "\n",
    "#adding pad\n",
    "maxi=0\n",
    "for i in training_op_sect:\n",
    "    maxi=max(maxi,len(i))\n",
    "training_op_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_op.transform([STOP])) for i in training_op_sect]\n",
    "valid_op_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_op.transform([STOP])) for i in valid_op_sect]\n",
    "\n",
    "\n",
    "#int to onehot\n",
    "training_op_sectt=[[onehoter2[i] for i in j] for j in training_op_sect0]\n",
    "valid_op_sectt=[[onehoter2[i] for i in j] for j in valid_op_sect0]\n",
    "\n",
    "training_op_sectt_nopad=[[onehoter2[i] for i in j] for j in training_op_sect]\n",
    "valid_op_sectt_nopad=[[onehoter2[i] for i in j] for j in valid_op_sect]\n",
    "\n",
    "\n",
    "del training_op_sec,valid_op_sec\n",
    "del training_op_sect,valid_op_sect\n",
    "#del training_op_sect0,valid_op_sect0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using sect\n",
    "embedding_size=256\n",
    "lstm_units=512\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_seq = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "target_seq = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "source_seq_len = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "target_seq_len = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "no_start_target_seq = tf.placeholder(shape=(None, None), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_encode = tf.get_variable(\n",
    "    name=\"embedding_matrix_en\",\n",
    "    shape=[ip_vocab_size, embedding_size],\n",
    "    dtype=tf.float32)\n",
    "embedding_matrix_decode = tf.get_variable(\n",
    "    name=\"embedding_matrix_de\",\n",
    "    shape=[op_vocab_size, embedding_size],\n",
    "    dtype=tf.float32)\n",
    "source_seq_embedded = tf.nn.embedding_lookup(embedding_matrix_encode, source_seq) \n",
    "decoder_input_embedded = tf.nn.embedding_lookup(embedding_matrix_decode, target_seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (encoder_state_fw,encoder_state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "    tf.contrib.rnn.LSTMCell(lstm_units),tf.contrib.rnn.LSTMCell(lstm_units),\n",
    "    source_seq_embedded,\n",
    "    sequence_length=source_seq_len,\n",
    "    dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_state_fw.c, encoder_state_bw.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_state_fw.h, encoder_state_bw.h), 1)\n",
    "\n",
    "encoder_final_state = tf.contrib.rnn.LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_state=encoder_final_state#tf.layers.dropout(inputs=encoder_final_state,rate=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.layers.Dense(op_vocab_size)\n",
    "\n",
    "decoder_cell=tf.contrib.rnn.LSTMCell(lstm_units*2)\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(decoder_input_embedded,target_seq_len)\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, encoder_state,output_layer=output_layer)#,output_layer=projection_layer)\n",
    "outputs, state, seq_len = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "logits = outputs.rnn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper2 = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_matrix_decode,tf.fill([BATCH],\n",
    "                                                    np.int32(pre_op.transform([GO])[0])),\n",
    "                                                   np.int32(pre_op.transform([STOP])[0]))\n",
    "\n",
    "decoder2 = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper2, encoder_state,output_layer=output_layer)#,output_layer=projection_layer)\n",
    "outputs, state, seq_len = tf.contrib.seq2seq.dynamic_decode(decoder2,maximum_iterations=50)\n",
    "\n",
    "#outputs, _,_ = tf.contrib.seq2seq.dynamic_decode(decoder2, maximum_iterations=50)\n",
    "translations = outputs.rnn_output\n",
    "trs=outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=no_start_target_seq,logits=logits)\n",
    "loss=tf.reduce_sum(cross_entropy)\n",
    "train = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxtlen=max(training_op_sec_len)\n",
    "maxvlen=max(valid_op_sec_len)\n",
    "t_newlen=[maxtlen-1 for i in range(len(training_op_sec_len))]\n",
    "v_newlen=[maxtlen for i in range(len(valid_op_sec_len))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 531544.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c995e27470a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                               \u001b[0msource_seq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtraining_ip_sec_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                 \u001b[0mtarget_seq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt_newlen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                                 \u001b[0mno_start_target_seq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op_sect0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                                                 })\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    for i in range(len(training_ip_sectt)/BATCH):\n",
    "        start=i*BATCH\n",
    "        stop=(i+1)*BATCH\n",
    "        \n",
    "        _,lost=sess.run([train,loss],feed_dict={source_seq:training_ip_sect0[start:stop],\n",
    "                                                target_seq:training_op_sect0[start:stop],\n",
    "                                              source_seq_len:training_ip_sec_len[start:stop],\n",
    "                                                target_seq_len:t_newlen[start:stop],\n",
    "                                                no_start_target_seq:np.array(training_op_sect0[start:stop])[:,1:]\n",
    "                                                })\n",
    "        print stop/BATCH,lost\n",
    "        #print 'Epoch :%d step:%d training loss:%f validation loss:%f'%(j,i,lost)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=sess.run([translations,trs],feed_dict={source_seq:valid_ip_sect0[0:BATCH],\n",
    "                                               source_seq_len:valid_ip_sec_len[0:BATCH],\n",
    "                                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['42', '.', '8am', '42', '1900', '1800', '1800', '1800',\n",
       "       'Occasional', 'Occasional', '7am', '2200', '2200', '2200', '9pm',\n",
       "       '9pm', '9pm', '9pm', '9pm', '9pm', '9pm', '9pm', '9pm', 'Very',\n",
       "       'Very', '5400', '5400', 'decreasing', '25', '25', '80', '3am',\n",
       "       '80', 'freezing', '72', '1300', '-11', '-11', '5100', '5100',\n",
       "       '5100', '5100', '5100', '5100', '5100', '5100', '5100', '5100',\n",
       "       '5100', '5100'], dtype='|S13')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_op.inverse_transform(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['<go>', 'Sunny', ',', 'with', 'a', 'high', 'near', '46', '.',\n",
       "       'West', 'wind', 'between', '6', 'and', '9', 'mph', '.', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "       '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>'],\n",
       "      dtype='|S13')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_op.inverse_transform(valid_op_sect0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,stop=0,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "v_newlen[1000:1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=sess.run([target_seq,logits],feed_dict={source_seq:training_ip_sect0[start:stop],\n",
    "                                                target_seq:training_op_sect0[start:stop],\n",
    "                                              source_seq_len:training_ip_sec_len[start:stop],\n",
    "                                                target_seq_len:v_newlen[start:stop],\n",
    "                                                \n",
    "                                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['<go>', 'A', '50', 'percent', 'chance', 'of', 'showers', '.',\n",
       "        'Cloudy', ',', 'with', 'a', 'low', 'around', '47', '.', 'East',\n",
       "        'wind', 'around', '5', 'mph', '.', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>'],\n",
       "       ['A', '50', 'chance', 'chance', 'of', 'showers', '.', 'Cloudy',\n",
       "        ',', 'with', 'a', 'low', 'around', '47', '.', 'East', 'wind',\n",
       "        'around', '5', 'mph', '.', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>']],\n",
       "      dtype='|S13')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_op.inverse_transform([x[1],np.argmax(y[1],axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_ip_sect0[1000:1020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_newlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[429, 573, 206, ..., 410, 410, 410],\n",
       "       [411, 335, 548, ..., 410, 410, 410],\n",
       "       [418, 206, 594, ..., 410, 410, 410],\n",
       "       ...,\n",
       "       [442, 232, 428, ..., 410, 410, 410],\n",
       "       [429, 480, 206, ..., 410, 410, 410],\n",
       "       [411, 564, 478, ..., 410, 410, 410]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(training_op_sect0)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/batch-1000'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saver = tf.train.Saver()\n",
    "#saver.save(sess, './model/batch',global_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/batch-1000\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.import_meta_graph('./model/batch-1000.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./model/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
