{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO='<go>'\n",
    "STOP='<stop>'\n",
    "PAD='<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breaker(listofsentences,start=True):\n",
    "    '''returns a list of list of strings'''\n",
    "    if listofsentences==None:\n",
    "        return None\n",
    "    lst=[]\n",
    "    for i in listofsentences:\n",
    "        t=i.split()\n",
    "        if start:\n",
    "            t=[GO]+t+[PAD]\n",
    "        else:\n",
    "            t=t+[STOP]\n",
    "        lst.append(t)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(data='train'):\n",
    "    COMBINED='./WeatherGov/'+data+'/'+data+'.combined'\n",
    "    SUMMARY='./WeatherGov/'+data+'/summaries.txt'\n",
    "    op_sec=None\n",
    "    with open(COMBINED) as f:\n",
    "        content = f.readlines()\n",
    "    ip_sec = [x.strip() for x in content]\n",
    "    if data!='test':\n",
    "        with open(SUMMARY) as f:\n",
    "            content = f.readlines()\n",
    "        op_sec = [x.strip() for x in content]\n",
    "        \n",
    "    return breaker(ip_sec,True),breaker(op_sec,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listofwords(data):\n",
    "    '''takes a list of sentences nd returns vocab'''\n",
    "    a=[]\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            if j not in a:\n",
    "                a.append(j)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ip_sec,training_op_sec=read('train')\n",
    "valid_ip_sec,valid_op_sec=read('dev')\n",
    "test_ip_sec,_=read('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_vocab=listofwords(training_ip_sec)\n",
    "op_vocab=listofwords(training_op_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ip=pre.LabelEncoder()\n",
    "pre_ip.fit(ip_vocab)\n",
    "\n",
    "onehoter=np.identity(len(pre_ip.classes_))\n",
    "\n",
    "#word to int\n",
    "training_ip_sect=[pre_ip.transform(i) for i in training_ip_sec]\n",
    "valid_ip_sect=[pre_ip.transform(i) for i in valid_ip_sec]\n",
    "test_ip_sect=[pre_ip.transform(i) for i in test_ip_sec]\n",
    "\n",
    "#adding pads\n",
    "maxi=0\n",
    "for i in training_ip_sect:\n",
    "    maxi=max(maxi,len(i))\n",
    "training_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in training_ip_sect]\n",
    "valid_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in valid_ip_sect]\n",
    "test_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in test_ip_sect]\n",
    "\n",
    "#making them onehot\n",
    "training_ip_sectt=[[onehoter[i] for i in j] for j in training_ip_sect0]\n",
    "valid_ip_sectt=[[onehoter[i] for i in j] for j in valid_ip_sect0]\n",
    "test_ip_sectt=[[onehoter[i] for i in j] for j in test_ip_sect0]\n",
    "\n",
    "\n",
    "\n",
    "pre_op=pre.LabelEncoder()\n",
    "pre_op.fit(op_vocab)\n",
    "\n",
    "onehoter2=np.identity(len(pre_op.classes_))\n",
    "\n",
    "#word to int\n",
    "training_op_sect=[pre_op.transform(i) for i in training_op_sec]\n",
    "valid_op_sect=[pre_op.transform(i) for i in valid_op_sec]\n",
    "\n",
    "#adding pad\n",
    "maxi=0\n",
    "for i in training_op_sect:\n",
    "    maxi=max(maxi,len(i))\n",
    "training_op_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_op.transform([STOP])) for i in training_op_sect]\n",
    "valid_op_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_op.transform([STOP])) for i in valid_op_sect]\n",
    "\n",
    "#int to onehot\n",
    "training_op_sectt=[[onehoter2[i] for i in j] for j in training_op_sect0]\n",
    "valid_op_sectt=[[onehoter2[i] for i in j] for j in valid_op_sect0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([205, 130, 342, 272, 337, 356,  27, 212,   1, 388, 251, 320, 261,\n",
       "       121,  27, 214, 385, 261, 129, 328,  27, 204])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_op_sect[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeder\n",
    "tf.reset_default_graph()\n",
    "def INEMBED(ip):\n",
    "    embed=tf.layers.dense(inputs=ip,activation=tf.nn.relu,units=256)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "\n",
    "def ENCODER(ip):\n",
    "    lstmcell=tf.contrib.rnn.BasicLSTMCell(num_units=512)\n",
    "    #t, states  = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstmcell, cell_bw=lstmcell,dtype=tf.float32,inputs=ip)\n",
    "    _,states=tf.nn.dynamic_rnn(cell=lstmcell,inputs=ip,dtype=tf.float32)\n",
    "    \n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def DECODER(ip,op_emb,op_len,vocab):\n",
    "    projection_layer=tf.layers.Dense(vocab)\n",
    "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(512)\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(op_emb,op_len)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, ip,output_layer=projection_layer)\n",
    "    outputs,_,_ = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "    logits = outputs.rnn_output\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DECODER(encoded,emb):\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(512)\n",
    "\n",
    "    decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(decoder_cell, \n",
    "                                                             emb,\n",
    "                                                             initial_state=encoded,dtype=tf.float32)\n",
    "    return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OUTEMBED(ip,vocab):\n",
    "    decoder_logits = tf.layers.dense(ip,units=vocab,activation=tf.nn.relu)\n",
    "\n",
    "    #decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "    return decoder_logits\n",
    "    #return decoder_logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip=tf.placeholder(dtype=tf.float32,shape=(None,None,len(pre_ip.classes_)))\n",
    "#max-time,bs,ip\n",
    "op_emb=tf.placeholder(dtype=tf.float32,shape=(None,None,len(pre_op.classes_)))\n",
    "op_len=tf.placeholder(dtype=tf.int32,shape=(None,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m=ENCODER(INEMBED(ip))\n",
    "n=OUTEMBED(DECODER(m,op_emb),vocab=len(pre_op.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 18, 390)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([training_op_sectt[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 390)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mm=sess.run(n,feed_dict={ip:[training_ip_sectt[0]],op_emb:[training_op_sectt[0]]})\n",
    "mm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre_op.classes_)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxi=0\n",
    "for i in range(len(training_ip_sectt)):\n",
    "    maxi=max(maxi,len(training_ip_sectt[i]))\n",
    "print maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([150, 184, 186, 103, 175,  62, 174,  82, 173,  94, 187, 186, 103,\n",
       "       175,  33, 174,  57, 173,  85, 189, 186, 103, 175, 101, 174, 127,\n",
       "       173,  37, 178,  34, 188, 186, 103, 176, 164, 172, 186, 103, 175,\n",
       "        33, 174,  33, 173,  33, 181, 186, 103, 177,  91, 181, 186, 104,\n",
       "       177,  35, 181, 186, 102, 177,  61, 181, 186, 139, 177,  91, 181,\n",
       "       186,  43, 177,  91, 179, 186, 103, 175,  33, 174,  53, 173,  41])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ip_sect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi=0\n",
    "for i in training_ip_sect:\n",
    "    maxi=max(maxi,len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst=np.copy(training_ip_sect[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=np.pad(tst,(0,maxi-len(tst)),'constant',constant_values=pre_ip.transform([PAD]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.array(training_op_sectt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 89, 390)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
