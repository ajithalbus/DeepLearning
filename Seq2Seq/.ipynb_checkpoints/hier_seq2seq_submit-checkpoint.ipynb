{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, GRUCell\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel():\n",
    "    def foo(self):\n",
    "        return self.logits\n",
    "    def __init__(self, mode, src_vocab_size, tgt_vocab_size, embedding_size, batch_size, learning_rate, beam_search = False,\n",
    "                 beam_width = 5):\n",
    "        self.mode = mode\n",
    "        self.beam_search = beam_search\n",
    "        self.beam_width = beam_width\n",
    "        self.learning_rate = learning_rate\n",
    "        self._init_placeholders()\n",
    "        self._init_embeddings(src_vocab_size, tgt_vocab_size, embedding_size)\n",
    "        self._init_bidirectional_encoder()\n",
    "        self._init_decoder(tgt_vocab_size, batch_size)\n",
    "        self._init_optimizer(batch_size)\n",
    "        \n",
    "    def _init_debug(self):\n",
    "        self.encoder_inputs = tf.Variable(np.array([[3, 3, 3, 3],[3, 3, 0, 0],[3, 0, 0, 0]]),dtype=np.int32)\n",
    "        self.encoder_input_length = tf.constant(np.array([4,2,1]),dtype=np.int32)\n",
    "\n",
    "        self.decoder_inputs = tf.Variable(np.array([[1, 3, 4, 2, 0],[1, 4, 2, 0, 0],[1, 3, 3, 3, 2]]),dtype=np.int32)\n",
    "        self.decoder_input_length = tf.constant(np.array([4,3,5]),dtype=np.int32)\n",
    "        self.decoder_outputs = tf.Variable(np.array([[1, 3, 4, 2, 0],[1, 4, 2, 0, 0],[1, 3, 3, 3, 2]]),dtype=np.int32)\n",
    "\n",
    "    def _init_placeholders(self):\n",
    "        self.encoder1_inputs = tf.placeholder(\n",
    "            shape=(None, None),\n",
    "            dtype=tf.int32,\n",
    "            name='encoder_inputs',\n",
    "        )\n",
    "        self.encoder2_inputs = tf.placeholder(\n",
    "            shape=(None, None),\n",
    "            dtype=tf.int32,\n",
    "            name='encoder_inputs',\n",
    "        )\n",
    "        self.decoder_inputs = tf.placeholder(\n",
    "            shape=(None, None),\n",
    "            dtype=tf.int32,\n",
    "            name='decoder_inputs',\n",
    "        )\n",
    "        self.decoder_input_length = tf.placeholder(\n",
    "            shape=(None,),\n",
    "            dtype=tf.int32,\n",
    "            name='decoder_input_length',\n",
    "        )\n",
    "        self.decoder_outputs = tf.placeholder(\n",
    "            shape=(None,None),\n",
    "            dtype=tf.int32,\n",
    "            name='decoder_outputs',\n",
    "        )\n",
    "        self.training=tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    def _init_embeddings(self, src_vocab_size, tgt_vocab_size, embedding_size):\n",
    "        with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE) as scope:\n",
    "\n",
    "            # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n",
    "            sqrt3 = math.sqrt(3)\n",
    "            initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "\n",
    "            self.encoder_embedding_matrix = tf.get_variable(\n",
    "                name=\"encoder_embedding_matrix\",\n",
    "                shape=[src_vocab_size, embedding_size],\n",
    "                initializer=initializer,\n",
    "                dtype=tf.float32)\n",
    "            \n",
    "            self.decoder_embedding_matrix = tf.get_variable(\n",
    "                name=\"decoder_embedding_matrix\",\n",
    "                shape=[tgt_vocab_size, embedding_size],\n",
    "                initializer=initializer,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            self.decoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "                self.decoder_embedding_matrix, self.decoder_inputs)  \n",
    "            \n",
    "    def _init_bidirectional_encoder(self):\n",
    "        \n",
    "        with tf.variable_scope(\"BidirectionalEncoder\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            \n",
    "            encoder_cell1 = LSTMCell(512)\n",
    "            encoder_cell2 = LSTMCell(512)\n",
    "            \n",
    "            self.encoder_states = []\n",
    "            \n",
    "            for i in range(20):\n",
    "                self.encoder1_inputs_embedded = tf.nn.embedding_lookup(\n",
    "                    self.encoder_embedding_matrix, self.encoder1_inputs[i*10:(i*10)+10,:])\n",
    "\n",
    "                self.encoder2_inputs_embedded = tf.nn.embedding_lookup(\n",
    "                    self.encoder_embedding_matrix, self.encoder2_inputs[i,:])\n",
    "\n",
    "                ((encoder_fw_outputs,\n",
    "                  encoder_bw_outputs),\n",
    "                 (encoder_fw_state,\n",
    "                  encoder_bw_state)) = (\n",
    "                    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell1,\n",
    "                                                    cell_bw=encoder_cell1,\n",
    "                                                    inputs=self.encoder1_inputs_embedded,\n",
    "                                                    time_major=True,\n",
    "                                                    dtype=tf.float32)\n",
    "                    )\n",
    "\n",
    "                \n",
    "                if isinstance(encoder_fw_state, LSTMStateTuple):\n",
    "\n",
    "                    encoder_state_c = tf.concat(\n",
    "                        (encoder_fw_state.c, encoder_bw_state.c), 1, name='bidirectional_concat_c')\n",
    "                    encoder_state_h = tf.concat(\n",
    "                        (encoder_fw_state.h, encoder_bw_state.h), 1, name='bidirectional_concat_h')\n",
    "                    self.encoder_state = LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "                elif isinstance(encoder_fw_state, tf.Tensor):\n",
    "                    self.encoder_state = tf.concat((encoder_fw_state, encoder_bw_state), 2, name='bidirectional_concat')\n",
    "\n",
    "                #self.encoder_states.append(tf.layers.dropout(self.encoder_state.c,training=self.training,rate=0.5))\n",
    "                self.encoder_states.append(tf.layers.dropout(tf.concat((self.encoder_state.c, self.encoder2_inputs_embedded), 1),\n",
    "                                                            training=self.training,rate=0.5))\n",
    "                \n",
    "            self.encoder_states = tf.stack(self.encoder_states)\n",
    "            print(self.encoder_states.get_shape())\n",
    "\n",
    "        with tf.variable_scope(\"BidirectionalEncoder1\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            \n",
    "            ((encoder_fw_outputs,\n",
    "              encoder_bw_outputs),\n",
    "             (encoder_fw_state,\n",
    "              encoder_bw_state)) = (\n",
    "                tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell2,\n",
    "                                                cell_bw=encoder_cell2,\n",
    "                                                inputs=self.encoder_states,\n",
    "                                                time_major=True,\n",
    "                                                dtype=tf.float32)\n",
    "                )\n",
    "            \n",
    "            if isinstance(encoder_fw_state, LSTMStateTuple):\n",
    "\n",
    "                encoder_state_c = tf.concat(\n",
    "                    (encoder_fw_state.c, encoder_bw_state.c), 1, name='bidirectional_concat_c')\n",
    "                encoder_state_h = tf.concat(\n",
    "                    (encoder_fw_state.h, encoder_bw_state.h), 1, name='bidirectional_concat_h')\n",
    "                self.encoder_state = LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "            elif isinstance(encoder_fw_state, tf.Tensor):\n",
    "                self.encoder_state = tf.concat((encoder_fw_state, encoder_bw_state), 2, name='bidirectional_concat')\n",
    "            self.encoder3_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "            #self.encoder_state = tf.layers.dropout(self.encoder_state.c,training=self.training,rate=0.5)\n",
    "                \n",
    "    def _init_decoder(self, tgt_vocab_size, batch_size):\n",
    "        with tf.variable_scope(\"Decoder\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.output_layer = layers_core.Dense(\n",
    "                                tgt_vocab_size, use_bias=False)\n",
    "            decoder_cell = LSTMCell(1024)\n",
    "\n",
    "            if self.mode == \"train\":\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                            self.decoder_inputs_embedded, self.decoder_input_length, time_major=True)\n",
    "\n",
    "                # Decoder\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    decoder_cell, helper, self.encoder_state, output_layer=self.output_layer)\n",
    "                # Dynamic decoding\n",
    "                outputs, _ , _ = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "                self.logits = outputs.rnn_output\n",
    "                self.op = outputs.sample_id\n",
    "\n",
    "            else:\n",
    "                helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                            self.decoder_embedding_matrix,\n",
    "                            start_tokens=tf.fill([batch_size], 0),\n",
    "                            end_token=1)\n",
    "\n",
    "                # Decoder\n",
    "                if self.beam_search:\n",
    "                    self.encoder_state = tf.contrib.seq2seq.tile_batch(\n",
    "                        self.encoder_state, multiplier=self.beam_width)\n",
    "\n",
    "                    # Define a beam-search decoder\n",
    "                    decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                            cell=decoder_cell,\n",
    "                            embedding=self.decoder_embedding_matrix,\n",
    "                            start_tokens=tf.fill([batch_size], 0),\n",
    "                            end_token=1,\n",
    "                            initial_state=self.encoder_state,\n",
    "                            beam_width=self.beam_width,\n",
    "                            output_layer=self.output_layer,\n",
    "                            length_penalty_weight=0.0)\n",
    "\n",
    "                else:\n",
    "                    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                        decoder_cell, helper, self.encoder_state,\n",
    "                        output_layer=self.output_layer)\n",
    "                # Dynamic decoding\n",
    "                outputs, _ , _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, maximum_iterations=100)\n",
    "                if self.beam_search:\n",
    "                    self.op = outputs.predicted_ids\n",
    "                else:\n",
    "                    self.op = outputs.sample_id\n",
    "    \n",
    "    def _init_optimizer(self,batch_size):\n",
    "        if not self.beam_search and self.mode == 'train': \n",
    "            with tf.variable_scope(\"Optimizer\", reuse=tf.AUTO_REUSE) as scope:\n",
    "                #target_output = tf.transpose(self.decoder_outputs)\n",
    "                max_time = tf.reduce_max(self.decoder_input_length)\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.decoder_outputs, logits=self.logits)\n",
    "                #target_weights = tf.sequence_mask(\n",
    "                #    self.decoder_input_length, max_time, dtype=self.logits.dtype)\n",
    "                #target_weights = tf.transpose(target_weights)\n",
    "                self.loss = tf.reduce_sum(crossent)\n",
    "                self.train_op = tf.train.AdamOptimizer().minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def readFile(fileName,word2id):\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.split() for x in content]\n",
    "    i = len(word2id)\n",
    "    for line in content:\n",
    "        for word in line:\n",
    "            if word not in word2id:\n",
    "                word2id[word] = i\n",
    "                i+=1\n",
    "    return content,word2id\n",
    "\n",
    "def sequence_converter(content, word2id):\n",
    "    \n",
    "    input_lengths = np.zeros(len(content))\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        for j in range(len(content[i])):\n",
    "            content[i][j] = word2id[content[i][j]]\n",
    "            \n",
    "    return np.array(content).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vocab = {}\n",
    "\n",
    "encoder1_input, encoder_vocab = readFile('WeatherGov/train/train.proc',encoder_vocab)\n",
    "\n",
    "encoder2_input, encoder_vocab = readFile('WeatherGov/train/train.field',encoder_vocab)\n",
    "\n",
    "encoder1_input = sequence_converter(encoder1_input, encoder_vocab)\n",
    "encoder2_input = sequence_converter(encoder2_input, encoder_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_readFile(fileName):\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.split() for x in content]\n",
    "    word2id = {}\n",
    "    word2id['sos'] = 0\n",
    "    word2id['eos'] = 1\n",
    "    i = 2\n",
    "    for line in content:\n",
    "        for word in line:\n",
    "            if word not in word2id:\n",
    "                word2id[word] = i\n",
    "                i+=1\n",
    "    return content,word2id\n",
    "\n",
    "def d_sequence_converter(content, word2id, decoder_inputs = False, no_pad = False):\n",
    "    \n",
    "    input_max_length = 0\n",
    "    input_lengths = np.zeros(len(content))\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        for j in range(len(content[i])):\n",
    "            input_lengths[i] = len(content[i])\n",
    "            input_max_length = max(input_max_length,len(content[i]))\n",
    "\n",
    "    for i in range(len(content)):\n",
    "        if decoder_inputs:\n",
    "            content[i].insert(0,'sos')\n",
    "        if not no_pad:\n",
    "            while len(content[i]) <= input_max_length:\n",
    "                content[i].append('eos')\n",
    "\n",
    "    for i in range(len(content)):\n",
    "        for j in range(len(content[i])):\n",
    "            content[i][j] = word2id[content[i][j]]\n",
    "            \n",
    "    if not no_pad:    \n",
    "        return np.array(content).T, input_lengths\n",
    "    else:\n",
    "        return content, input_lengths\n",
    "\n",
    "decoder_op, decoder_vocab = d_readFile('WeatherGov/train/summaries.txt')\n",
    "\n",
    "decoder_target, decoder_target_lengths = d_sequence_converter(decoder_op, decoder_vocab, no_pad = True)\n",
    "\n",
    "decoder_op, _ = d_readFile('WeatherGov/train/summaries.txt')\n",
    "decoder_input, _ = d_sequence_converter(decoder_op, decoder_vocab, True)\n",
    "\n",
    "decoder_id2word = {}\n",
    "for i,j in decoder_vocab.iteritems():\n",
    "    decoder_id2word[j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_readFile(fileName):\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.split() for x in content]\n",
    "    return content\n",
    "\n",
    "def v_sequence_converter(content, word2id):\n",
    "    \n",
    "    input_lengths = np.zeros(len(content))\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        for j in range(len(content[i])):\n",
    "            content[i][j] = word2id[content[i][j]]\n",
    "            \n",
    "    return np.array(content).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_encoder1_input = v_readFile('WeatherGov/dev/dev.proc')\n",
    "\n",
    "v_encoder2_input = v_readFile('WeatherGov/dev/dev.field')\n",
    "\n",
    "v_encoder1_input = v_sequence_converter(v_encoder1_input, encoder_vocab)\n",
    "v_encoder2_input = v_sequence_converter(v_encoder2_input, encoder_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import evaluate\n",
    "\n",
    "def validate_bleu(model1, v_encoder1_input, v_encoder2_input, decoder_input, decoder_target_lengths, epoch):\n",
    "    bi = []\n",
    "    for b in range(0,20,bs):\n",
    "        max_len = np.max(decoder_target_lengths[b:b+bs])\n",
    "        x = []\n",
    "        for a in decoder_target[b:b+bs]:\n",
    "            x.append((a + [1] * int((max_len - len(a)))))\n",
    "        x= np.array(x).T\n",
    "        feed_dict = {model1.encoder1_inputs: v_encoder1_input[:,b:b+bs],\n",
    "                             model1.encoder2_inputs: v_encoder2_input[:,b:b+bs],\n",
    "                             model1.decoder_inputs: decoder_input[:,b:b+bs],\n",
    "                             model1.decoder_input_length: decoder_target_lengths[b:b+bs],\n",
    "                             model1.decoder_outputs: x,\n",
    "                             model1.training: False\n",
    "                            }\n",
    "        bi.append(sess.run(model1.op, feed_dict=feed_dict))\n",
    "        \n",
    "    f = open('op.txt','w+')\n",
    "\n",
    "    for b in bi:\n",
    "        for i in range(len(b[:,:,0])):\n",
    "            for j in range(len(b[i,:,0])):\n",
    "                if b[i,j,0] == -1 or b[i,j,0] == 1:\n",
    "                    break\n",
    "                else:\n",
    "                    f.write(decoder_id2word[b[i,j,0]]+' ')\n",
    "            f.write('\\n')\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    f = open('op_epoch'+str(epoch)+'.txt','w+')\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for lines in open(\"op.txt\"):\n",
    "        if i < 3528:\n",
    "            f.write(lines.strip()+'\\n')\n",
    "            i=i+1\n",
    "        else:\n",
    "            break\n",
    "    f.close()\n",
    "    \n",
    "    evaluate('op_epoch'+str(epoch)+'.txt', 'WeatherGov/dev/summaries.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(\n",
    "                     mode = \"train\",\n",
    "                     src_vocab_size=len(encoder_vocab), \n",
    "                     tgt_vocab_size=len(decoder_vocab), \n",
    "                     embedding_size=256, \n",
    "                     batch_size=bs, \n",
    "                     learning_rate = 0.001\n",
    "                    )\n",
    "\n",
    "model1 = Seq2SeqModel(\n",
    "                     mode = \"train\",\n",
    "                     src_vocab_size=len(encoder_vocab), \n",
    "                     tgt_vocab_size=len(decoder_vocab), \n",
    "                     embedding_size=256, \n",
    "                     batch_size=bs, \n",
    "                     learning_rate = 0.001,\n",
    "                     beam_search = True,\n",
    "                     beam_width = 5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "sess = tf.Session(config = config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6fab1824f2f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_encoder1_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_encoder2_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_target_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-709fc2daf41d>\u001b[0m in \u001b[0;36mvalidate_bleu\u001b[0;34m(model1, v_encoder1_input, v_encoder2_input, decoder_input, decoder_target_lengths, epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'op_epoch'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WeatherGov/WeatherGov/dev/summaries.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/e/Masters/Sem_2/Deep_Learning/Assignments/Programming/A4/main.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(annFile, resFile, phase_codename, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mcocoEval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcocoRes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcocoEval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Bleu Score: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Successfully completed the process of evaluation.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/e/Masters/Sem_2/Deep_Learning/Assignments/Programming/A4/main.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'tokenization...'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPTBTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mgts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/e/Masters/Sem_2/Deep_Learning/Assignments/Programming/A4/pycocoevalcap/tokenizer/ptbtokenizer.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, captions_for_image)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# ======================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         p_tokenizer = subprocess.Popen(cmd, cwd=path_to_jar_dirname, \\\n\u001b[0m\u001b[1;32m     54\u001b[0m                 stdout=subprocess.PIPE)\n\u001b[1;32m     55\u001b[0m         \u001b[0mtoken_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                 \u001b[0mp2cread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2cwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                                 errread, errwrite)\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# Preserve original exception in case os.close raises.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 \u001b[0mchild_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "print(validate_bleu(model1, v_encoder1_input, v_encoder2_input, decoder_input, decoder_target_lengths, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c7bd93142d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0me_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_encoder1_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_encoder2_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_target_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-709fc2daf41d>\u001b[0m in \u001b[0;36mvalidate_bleu\u001b[0;34m(model1, v_encoder1_input, v_encoder2_input, decoder_input, decoder_target_lengths, epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'op_epoch'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WeatherGov/WeatherGov/dev/summaries.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/e/Masters/Sem_2/Deep_Learning/Assignments/Programming/A4/main.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(annFile, resFile, phase_codename, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mcocoEval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcocoRes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcocoEval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Bleu Score: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Successfully completed the process of evaluation.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/e/Masters/Sem_2/Deep_Learning/Assignments/Programming/A4/main.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'tokenization...'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPTBTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mgts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/e/Masters/Sem_2/Deep_Learning/Assignments/Programming/A4/pycocoevalcap/tokenizer/ptbtokenizer.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, captions_for_image)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         p_tokenizer = subprocess.Popen(cmd, cwd=path_to_jar_dirname, \\\n\u001b[0;32m---> 53\u001b[0;31m                 stdout=subprocess.PIPE)\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mtoken_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                 \u001b[0mp2cread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2cwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                                 errread, errwrite)\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# Preserve original exception in case os.close raises.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 \u001b[0mchild_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for e in range(epochs):\n",
    "    e_loss = 0\n",
    "    for b in range(0,20,bs):\n",
    "        max_len = np.max(decoder_target_lengths[b:b+bs])\n",
    "        x = []\n",
    "        for a in decoder_target[b:b+bs]:\n",
    "            x+=((a + [1] * int((max_len - len(a)))))\n",
    "        x= np.array(x).T\n",
    "        x = np.reshape(x,(x.shape[0],1))\n",
    "        feed_dict = {model.encoder1_inputs: encoder1_input[:,b:b+bs],\n",
    "                     model.encoder2_inputs: encoder2_input[:,b:b+bs],\n",
    "                     model.decoder_inputs: decoder_input[:,b:b+bs],\n",
    "                     model.decoder_input_length: decoder_target_lengths[b:b+bs],\n",
    "                     model.decoder_outputs: x,\n",
    "                     model.training: True\n",
    "                    }\n",
    "        a, b1, c = sess.run([model.logits,model.loss,model.train_op], feed_dict=feed_dict)\n",
    "        e_loss += b1\n",
    "    print(e_loss)\n",
    "    print(validate_bleu(model1, v_encoder1_input, v_encoder2_input, decoder_input, decoder_target_lengths, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "v_encoder1_input = v_readFile('WeatherGov/WeatherGov/test/test.proc')\n",
    "\n",
    "v_encoder2_input = v_readFile('WeatherGov/WeatherGov/test/test.field')\n",
    "\n",
    "v_encoder1_input = v_sequence_converter(v_encoder1_input, encoder_vocab)\n",
    "v_encoder2_input = v_sequence_converter(v_encoder2_input, encoder_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, ?, 1280)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('op.txt','w+')\n",
    "\n",
    "for b in bi:\n",
    "    for i in range(len(b[:,:,0])):\n",
    "        for j in range(len(b[i,:,0])):\n",
    "            if b[i,j,0] == -1 or b[i,j,0] == 1:\n",
    "                break\n",
    "            else:\n",
    "                f.write(decoder_id2word[b[i,j,0]]+' ')\n",
    "        f.write('\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('op_processed_15.txt','w+')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for lines in open(\"op.txt\"):\n",
    "    if i < 3528:\n",
    "        f.write(lines.strip()+'\\n')\n",
    "        i=i+1\n",
    "    else:\n",
    "        break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
