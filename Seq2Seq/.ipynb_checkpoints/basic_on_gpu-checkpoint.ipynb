{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO='<go>'\n",
    "STOP='<stop>'\n",
    "PAD='<pad>'\n",
    "BATCH=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breaker(listofsentences,start=True):\n",
    "    '''returns a list of list of strings'''\n",
    "    if listofsentences==None:\n",
    "        return None\n",
    "    lst=[]\n",
    "    for i in listofsentences:\n",
    "        t=i.split()\n",
    "        if start:\n",
    "            t=[GO]+t+[PAD]\n",
    "        else:\n",
    "            t=t+[STOP]\n",
    "        lst.append(t)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(data='train'):\n",
    "    COMBINED='./WeatherGov/'+data+'/'+data+'.combined'\n",
    "    SUMMARY='./WeatherGov/'+data+'/summaries.txt'\n",
    "    op_sec=None\n",
    "    with open(COMBINED) as f:\n",
    "        content = f.readlines()\n",
    "    ip_sec = [x.strip() for x in content]\n",
    "    if data!='test':\n",
    "        with open(SUMMARY) as f:\n",
    "            content = f.readlines()\n",
    "        op_sec = [x.strip() for x in content]\n",
    "        \n",
    "    return breaker(ip_sec,True),breaker(op_sec,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listofwords(data):\n",
    "    '''takes a list of sentences nd returns vocab'''\n",
    "    a=[]\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            if j not in a:\n",
    "                a.append(j)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ip_sec,training_op_sec=read('train')\n",
    "valid_ip_sec,valid_op_sec=read('dev')\n",
    "test_ip_sec,_=read('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_vocab=listofwords(training_ip_sec)\n",
    "op_vocab=listofwords(training_op_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ip_sec_len=[len(i) for i in training_ip_sec]\n",
    "training_op_sec_len=[len(i) for i in training_op_sec]\n",
    "valid_ip_sec_len=[len(i) for i in valid_ip_sec]\n",
    "valid_op_sec_len=[len(i) for i in valid_op_sec]\n",
    "test_ip_sec_len=[len(i) for i in test_ip_sec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ip=pre.LabelEncoder()\n",
    "pre_ip.fit(ip_vocab)\n",
    "\n",
    "onehoter=np.identity(len(pre_ip.classes_))\n",
    "\n",
    "#word to int\n",
    "training_ip_sect=[pre_ip.transform(i) for i in training_ip_sec]\n",
    "valid_ip_sect=[pre_ip.transform(i) for i in valid_ip_sec]\n",
    "test_ip_sect=[pre_ip.transform(i) for i in test_ip_sec]\n",
    "\n",
    "#adding pads\n",
    "maxi=0\n",
    "for i in training_ip_sect:\n",
    "    maxi=max(maxi,len(i))\n",
    "training_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in training_ip_sect]\n",
    "valid_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in valid_ip_sect]\n",
    "test_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in test_ip_sect]\n",
    "\n",
    "#making them onehot\n",
    "training_ip_sectt=[[onehoter[i] for i in j] for j in training_ip_sect0]\n",
    "valid_ip_sectt=[[onehoter[i] for i in j] for j in valid_ip_sect0]\n",
    "test_ip_sectt=[[onehoter[i] for i in j] for j in test_ip_sect0]\n",
    "\n",
    "\n",
    "del training_ip_sec,valid_ip_sec,test_ip_sec\n",
    "del training_ip_sect,valid_ip_sect,test_ip_sect\n",
    "del training_ip_sect0,valid_ip_sect0,test_ip_sect0\n",
    "\n",
    "#OP sec\n",
    "pre_op=pre.LabelEncoder()\n",
    "pre_op.fit(op_vocab)\n",
    "\n",
    "onehoter2=np.identity(len(pre_op.classes_))\n",
    "\n",
    "#word to int\n",
    "training_op_sect=[pre_op.transform(i) for i in training_op_sec]\n",
    "valid_op_sect=[pre_op.transform(i) for i in valid_op_sec]\n",
    "\n",
    "#adding pad\n",
    "maxi=0\n",
    "for i in training_op_sect:\n",
    "    maxi=max(maxi,len(i))\n",
    "training_op_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_op.transform([STOP])) for i in training_op_sect]\n",
    "valid_op_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_op.transform([STOP])) for i in valid_op_sect]\n",
    "\n",
    "\n",
    "#int to onehot\n",
    "training_op_sectt=[[onehoter2[i] for i in j] for j in training_op_sect0]\n",
    "valid_op_sectt=[[onehoter2[i] for i in j] for j in valid_op_sect0]\n",
    "\n",
    "del training_op_sec,valid_op_sec\n",
    "del training_op_sect,valid_op_sect\n",
    "del training_op_sect0,valid_op_sect0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del training_ip_sectt,valid_ip_sectt,test_ip_sectt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeder\n",
    "tf.reset_default_graph()\n",
    "def INEMBED(ip):\n",
    "    embed=tf.layers.dense(inputs=ip,activation=tf.nn.relu,units=256)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "\n",
    "def ENCODER(ip,ip_len):\n",
    "    lstmcell=tf.contrib.rnn.BasicLSTMCell(num_units=512)\n",
    "    #t, states  = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstmcell, cell_bw=lstmcell,dtype=tf.float32,inputs=ip)\n",
    "    _,states=tf.nn.dynamic_rnn(cell=lstmcell,inputs=ip,dtype=tf.float32,sequence_length=ip_len)\n",
    "    \n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def DECODER(ip,op_emb,op_len,vocab):\n",
    "    projection_layer=tf.layers.Dense(vocab)\n",
    "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(512)\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(op_emb,op_len)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, ip,output_layer=projection_layer)\n",
    "    outputs,_,_ = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "    logits = outputs.rnn_output\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DECODER(encoded,emb,op_len):\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(512)\n",
    "\n",
    "    decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(decoder_cell, \n",
    "                                                                 emb,initial_state=encoded,dtype=tf.float32,sequence_length=op_len)\n",
    "    return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OUTEMBED(ip,vocab):\n",
    "    decoder_logits = tf.layers.dense(ip,units=vocab,activation=tf.nn.relu)\n",
    "\n",
    "    decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "    return decoder_logits,decoder_prediction\n",
    "    #return decoder_logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip=tf.placeholder(dtype=tf.float32,shape=(None,None,len(pre_ip.classes_)))\n",
    "ip_len=tf.placeholder(dtype=tf.int32,shape=(None,))\n",
    "op_emb=tf.placeholder(dtype=tf.float32,shape=(None,None,len(pre_op.classes_)))\n",
    "op_len=tf.placeholder(dtype=tf.int32,shape=(None,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m=ENCODER(INEMBED(ip),ip_len=ip_len)\n",
    "n,predicts=OUTEMBED(DECODER(m,op_emb,op_len=op_len),vocab=len(pre_op.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy=tf.nn.softmax_cross_entropy_with_logits(labels=op_emb,logits=n)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "train = tf.train.AdamOptimizer(0.01).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(sess,sec,seclen):\n",
    "    for i in range(seclen):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0 step:0 training loss:1.212499 validation loss:1.214987\n",
      "Epoch :1 step:0 training loss:1.126012 validation loss:1.128257\n",
      "Epoch :2 step:0 training loss:1.048332 validation loss:1.050387\n",
      "Epoch :3 step:0 training loss:0.979157 validation loss:0.981064\n",
      "Epoch :4 step:0 training loss:0.917991 validation loss:0.919782\n",
      "Epoch :5 step:0 training loss:0.864201 validation loss:0.865901\n",
      "Epoch :6 step:0 training loss:0.817076 validation loss:0.818702\n",
      "Epoch :7 step:0 training loss:0.775881 validation loss:0.777444\n",
      "Epoch :8 step:0 training loss:0.739897 validation loss:0.741405\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    for i in range(len(training_ip_sectt)/BATCH):\n",
    "        start=i*BATCH\n",
    "        stop=(i+1)*BATCH\n",
    "        _,lost=sess.run([train,loss],feed_dict={ip:training_ip_sectt[start:stop],op_emb:training_op_sectt[start:stop],\n",
    "                                              ip_len:training_ip_sec_len[start:stop],op_len:training_op_sec_len[start:stop]})\n",
    "        if i%5000==0:\n",
    "            losv=sess.run(loss,feed_dict={ip:valid_ip_sectt,op_emb:valid_op_sectt,\n",
    "                                              ip_len:valid_ip_sec_len,op_len:valid_op_sec_len})\n",
    "        \n",
    "            print 'Epoch :%d step:%d training loss:%f validation loss:%f'%(j,i,lost,losv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=sess.run(predicts,feed_dict={ip:valid_ip_sectt[:2],op_emb:valid_op_sectt[:2],ip_len:valid_ip_sec_len[:2],op_len:valid_op_sec_len[:2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=sess.run(predicts,feed_dict={ip:training_ip_sectt[3:6],op_emb:training_op_sectt[3:6],\n",
    "                              ip_len:training_ip_sec_len[3:6],op_len:training_op_sec_len[3:6]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxi=0\n",
    "for i in range(len(training_ip_sectt)):\n",
    "    maxi=max(maxi,len(training_ip_sectt[i]))\n",
    "print maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['Sunny', ',', 'with', 'a', '%', '%', '%', '.', 'West', 'wind',\n",
       "        'between', '6', 'and', '%', 'mph', '.', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>'],\n",
       "       ['Partly', 'cloudy', ',', 'with', 'a', 'low', 'around', '%', '.',\n",
       "        '%', 'wind', 'between', '5', 'and', '10', 'mph', '.', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>', '<stop>',\n",
       "        '<stop>', '<stop>', '<stop>', '<stop>', '<stop>']], dtype='|S13')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_op.inverse_transform(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Partly',\n",
       " 'cloudy',\n",
       " ',',\n",
       " 'with',\n",
       " 'a',\n",
       " 'low',\n",
       " 'around',\n",
       " '29',\n",
       " '.',\n",
       " 'Northwest',\n",
       " 'wind',\n",
       " 'between',\n",
       " '5',\n",
       " 'and',\n",
       " '10',\n",
       " 'mph',\n",
       " '.',\n",
       " '<stop>']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_op_sec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
