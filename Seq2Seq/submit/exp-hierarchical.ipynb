{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, GRUCell\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel():\n",
    "    def foo(self):\n",
    "        return self.logits\n",
    "    def __init__(self, mode, src_vocab_size, tgt_vocab_size, embedding_size, batch_size, learning_rate, beam_search = False,\n",
    "                 beam_width = 5):\n",
    "        self.mode = mode\n",
    "        self.beam_search = beam_search\n",
    "        self.beam_width = beam_width\n",
    "        self.learning_rate = learning_rate\n",
    "        self._init_placeholders()\n",
    "        self._init_embeddings(src_vocab_size, tgt_vocab_size, embedding_size)\n",
    "        self._init_bidirectional_encoder()\n",
    "        self._init_decoder(tgt_vocab_size, batch_size)\n",
    "        self._init_optimizer(batch_size)\n",
    "        \n",
    "    def _init_debug(self):\n",
    "        self.encoder_inputs = tf.Variable(np.array([[3, 3, 3, 3],[3, 3, 0, 0],[3, 0, 0, 0]]),dtype=np.int32)\n",
    "        self.encoder_input_length = tf.constant(np.array([4,2,1]),dtype=np.int32)\n",
    "\n",
    "        self.decoder_inputs = tf.Variable(np.array([[1, 3, 4, 2, 0],[1, 4, 2, 0, 0],[1, 3, 3, 3, 2]]),dtype=np.int32)\n",
    "        self.decoder_input_length = tf.constant(np.array([4,3,5]),dtype=np.int32)\n",
    "        self.decoder_outputs = tf.Variable(np.array([[1, 3, 4, 2, 0],[1, 4, 2, 0, 0],[1, 3, 3, 3, 2]]),dtype=np.int32)\n",
    "\n",
    "    def _init_placeholders(self):\n",
    "        self.encoder1_inputs = tf.placeholder(\n",
    "            shape=(None, None),\n",
    "            dtype=tf.int32,\n",
    "            name='encoder_inputs',\n",
    "        )\n",
    "        self.encoder2_inputs = tf.placeholder(\n",
    "            shape=(None, None),\n",
    "            dtype=tf.int32,\n",
    "            name='encoder_inputs',\n",
    "        )\n",
    "        self.decoder_inputs = tf.placeholder(\n",
    "            shape=(None, None),\n",
    "            dtype=tf.int32,\n",
    "            name='decoder_inputs',\n",
    "        )\n",
    "        self.decoder_input_length = tf.placeholder(\n",
    "            shape=(None,),\n",
    "            dtype=tf.int32,\n",
    "            name='decoder_input_length',\n",
    "        )\n",
    "        self.decoder_outputs = tf.placeholder(\n",
    "            shape=(None,None),\n",
    "            dtype=tf.int32,\n",
    "            name='decoder_outputs',\n",
    "        )\n",
    "        self.training=tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    def _init_embeddings(self, src_vocab_size, tgt_vocab_size, embedding_size):\n",
    "        with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE) as scope:\n",
    "\n",
    "            # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n",
    "            sqrt3 = math.sqrt(3)\n",
    "            initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "\n",
    "            self.encoder_embedding_matrix = tf.get_variable(\n",
    "                name=\"encoder_embedding_matrix\",\n",
    "                shape=[src_vocab_size, embedding_size],\n",
    "                initializer=initializer,\n",
    "                dtype=tf.float32)\n",
    "            \n",
    "            self.decoder_embedding_matrix = tf.get_variable(\n",
    "                name=\"decoder_embedding_matrix\",\n",
    "                shape=[tgt_vocab_size, embedding_size],\n",
    "                initializer=initializer,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            self.decoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "                self.decoder_embedding_matrix, self.decoder_inputs)  \n",
    "            \n",
    "    def _init_bidirectional_encoder(self):\n",
    "        \n",
    "        with tf.variable_scope(\"BidirectionalEncoder\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            \n",
    "            encoder_cell1 = LSTMCell(512)\n",
    "            encoder_cell2 = LSTMCell(512)\n",
    "            \n",
    "            self.encoder_states = []\n",
    "            \n",
    "            for i in range(20):\n",
    "                self.encoder1_inputs_embedded = tf.nn.embedding_lookup(\n",
    "                    self.encoder_embedding_matrix, self.encoder1_inputs[i*10:(i*10)+10,:])\n",
    "\n",
    "                self.encoder2_inputs_embedded = tf.nn.embedding_lookup(\n",
    "                    self.encoder_embedding_matrix, self.encoder2_inputs[i,:])\n",
    "\n",
    "                ((encoder_fw_outputs,\n",
    "                  encoder_bw_outputs),\n",
    "                 (encoder_fw_state,\n",
    "                  encoder_bw_state)) = (\n",
    "                    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell1,\n",
    "                                                    cell_bw=encoder_cell1,\n",
    "                                                    inputs=self.encoder1_inputs_embedded,\n",
    "                                                    time_major=True,\n",
    "                                                    dtype=tf.float32)\n",
    "                    )\n",
    "\n",
    "                \n",
    "                if isinstance(encoder_fw_state, LSTMStateTuple):\n",
    "\n",
    "                    encoder_state_c = tf.concat(\n",
    "                        (encoder_fw_state.c, encoder_bw_state.c), 1, name='bidirectional_concat_c')\n",
    "                    encoder_state_h = tf.concat(\n",
    "                        (encoder_fw_state.h, encoder_bw_state.h), 1, name='bidirectional_concat_h')\n",
    "                    self.encoder_state = LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "                elif isinstance(encoder_fw_state, tf.Tensor):\n",
    "                    self.encoder_state = tf.concat((encoder_fw_state, encoder_bw_state), 2, name='bidirectional_concat')\n",
    "\n",
    "                #self.encoder_states.append(tf.layers.dropout(self.encoder_state.c,training=self.training,rate=0.5))\n",
    "                self.encoder_states.append(tf.layers.dropout(tf.concat((self.encoder_state.c, self.encoder2_inputs_embedded), 1),\n",
    "                                                            training=self.training,rate=0.5))\n",
    "                \n",
    "            self.encoder_states = tf.stack(self.encoder_states)\n",
    "            print(self.encoder_states.get_shape())\n",
    "\n",
    "        with tf.variable_scope(\"BidirectionalEncoder1\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            \n",
    "            ((encoder_fw_outputs,\n",
    "              encoder_bw_outputs),\n",
    "             (encoder_fw_state,\n",
    "              encoder_bw_state)) = (\n",
    "                tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell2,\n",
    "                                                cell_bw=encoder_cell2,\n",
    "                                                inputs=self.encoder_states,\n",
    "                                                time_major=True,\n",
    "                                                dtype=tf.float32)\n",
    "                )\n",
    "            \n",
    "            if isinstance(encoder_fw_state, LSTMStateTuple):\n",
    "\n",
    "                encoder_state_c = tf.concat(\n",
    "                    (encoder_fw_state.c, encoder_bw_state.c), 1, name='bidirectional_concat_c')\n",
    "                encoder_state_h = tf.concat(\n",
    "                    (encoder_fw_state.h, encoder_bw_state.h), 1, name='bidirectional_concat_h')\n",
    "                self.encoder_state = LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "            elif isinstance(encoder_fw_state, tf.Tensor):\n",
    "                self.encoder_state = tf.concat((encoder_fw_state, encoder_bw_state), 2, name='bidirectional_concat')\n",
    "            self.encoder3_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "            #self.encoder_state = tf.layers.dropout(self.encoder_state.c,training=self.training,rate=0.5)\n",
    "                \n",
    "    def _init_decoder(self, tgt_vocab_size, batch_size):\n",
    "        with tf.variable_scope(\"Decoder\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.output_layer = layers_core.Dense(\n",
    "                                tgt_vocab_size, use_bias=False)\n",
    "            decoder_cell = LSTMCell(1024)\n",
    "\n",
    "            if self.mode == \"train\":\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                            self.decoder_inputs_embedded, self.decoder_input_length, time_major=True)\n",
    "\n",
    "                # Decoder\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    decoder_cell, helper, self.encoder_state, output_layer=self.output_layer)\n",
    "                # Dynamic decoding\n",
    "                outputs, _ , _ = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "                self.logits = outputs.rnn_output\n",
    "                self.op = outputs.sample_id\n",
    "\n",
    "            else:\n",
    "                helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                            self.decoder_embedding_matrix,\n",
    "                            start_tokens=tf.fill([batch_size], 0),\n",
    "                            end_token=1)\n",
    "\n",
    "                # Decoder\n",
    "                if self.beam_search:\n",
    "                    self.encoder_state = tf.contrib.seq2seq.tile_batch(\n",
    "                        self.encoder_state, multiplier=self.beam_width)\n",
    "\n",
    "                    # Define a beam-search decoder\n",
    "                    decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                            cell=decoder_cell,\n",
    "                            embedding=self.decoder_embedding_matrix,\n",
    "                            start_tokens=tf.fill([batch_size], 0),\n",
    "                            end_token=1,\n",
    "                            initial_state=self.encoder_state,\n",
    "                            beam_width=self.beam_width,\n",
    "                            output_layer=self.output_layer,\n",
    "                            length_penalty_weight=0.0)\n",
    "\n",
    "                else:\n",
    "                    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                        decoder_cell, helper, self.encoder_state,\n",
    "                        output_layer=self.output_layer)\n",
    "                # Dynamic decoding\n",
    "                outputs, _ , _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, maximum_iterations=100)\n",
    "                if self.beam_search:\n",
    "                    self.op = outputs.predicted_ids\n",
    "                else:\n",
    "                    self.op = outputs.sample_id\n",
    "    \n",
    "    def _init_optimizer(self,batch_size):\n",
    "        if not self.beam_search and self.mode == 'train': \n",
    "            with tf.variable_scope(\"Optimizer\", reuse=tf.AUTO_REUSE) as scope:\n",
    "                #target_output = tf.transpose(self.decoder_outputs)\n",
    "                max_time = tf.reduce_max(self.decoder_input_length)\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.decoder_outputs, logits=self.logits)\n",
    "                #target_weights = tf.sequence_mask(\n",
    "                #    self.decoder_input_length, max_time, dtype=self.logits.dtype)\n",
    "                #target_weights = tf.transpose(target_weights)\n",
    "                self.loss = tf.reduce_sum(crossent)\n",
    "                self.train_op = tf.train.AdamOptimizer().minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def readFile(fileName,word2id):\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.split() for x in content]\n",
    "    i = len(word2id)\n",
    "    for line in content:\n",
    "        for word in line:\n",
    "            if word not in word2id:\n",
    "                word2id[word] = i\n",
    "                i+=1\n",
    "    return content,word2id\n",
    "\n",
    "def sequence_converter(content, word2id):\n",
    "    \n",
    "    input_lengths = np.zeros(len(content))\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        for j in range(len(content[i])):\n",
    "            content[i][j] = word2id[content[i][j]]\n",
    "            \n",
    "    return np.array(content).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vocab = {}\n",
    "\n",
    "encoder1_input, encoder_vocab = readFile('WeatherGov/WeatherGov/train/train.proc',encoder_vocab)\n",
    "\n",
    "encoder2_input, encoder_vocab = readFile('WeatherGov/WeatherGov/train/train.field',encoder_vocab)\n",
    "\n",
    "encoder1_input = sequence_converter(encoder1_input, encoder_vocab)\n",
    "encoder2_input = sequence_converter(encoder2_input, encoder_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_readFile(fileName):\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.split() for x in content]\n",
    "    word2id = {}\n",
    "    word2id['sos'] = 0\n",
    "    word2id['eos'] = 1\n",
    "    i = 2\n",
    "    for line in content:\n",
    "        for word in line:\n",
    "            if word not in word2id:\n",
    "                word2id[word] = i\n",
    "                i+=1\n",
    "    return content,word2id\n",
    "\n",
    "def d_sequence_converter(content, word2id, decoder_inputs = False, no_pad = False):\n",
    "    \n",
    "    input_max_length = 0\n",
    "    input_lengths = np.zeros(len(content))\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        for j in range(len(content[i])):\n",
    "            input_lengths[i] = len(content[i])\n",
    "            input_max_length = max(input_max_length,len(content[i]))\n",
    "\n",
    "    for i in range(len(content)):\n",
    "        if decoder_inputs:\n",
    "            content[i].insert(0,'sos')\n",
    "        if not no_pad:\n",
    "            while len(content[i]) <= input_max_length:\n",
    "                content[i].append('eos')\n",
    "\n",
    "    for i in range(len(content)):\n",
    "        for j in range(len(content[i])):\n",
    "            content[i][j] = word2id[content[i][j]]\n",
    "            \n",
    "    if not no_pad:    \n",
    "        return np.array(content).T, input_lengths\n",
    "    else:\n",
    "        return content, input_lengths\n",
    "\n",
    "decoder_op, decoder_vocab = d_readFile('WeatherGov/WeatherGov/train/summaries.txt')\n",
    "\n",
    "decoder_target, decoder_target_lengths = d_sequence_converter(decoder_op, decoder_vocab, no_pad = True)\n",
    "\n",
    "decoder_op, _ = d_readFile('WeatherGov/WeatherGov/train/summaries.txt')\n",
    "decoder_input, _ = d_sequence_converter(decoder_op, decoder_vocab, True)\n",
    "\n",
    "decoder_id2word = {}\n",
    "for i,j in decoder_vocab.iteritems():\n",
    "    decoder_id2word[j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_readFile(fileName):\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.split() for x in content]\n",
    "    return content\n",
    "\n",
    "def v_sequence_converter(content, word2id):\n",
    "    \n",
    "    input_lengths = np.zeros(len(content))\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        for j in range(len(content[i])):\n",
    "            content[i][j] = word2id[content[i][j]]\n",
    "            \n",
    "    return np.array(content).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_encoder1_input = v_readFile('WeatherGov/WeatherGov/dev/dev.proc')\n",
    "\n",
    "v_encoder2_input = v_readFile('WeatherGov/WeatherGov/dev/dev.field')\n",
    "\n",
    "v_encoder1_input = v_sequence_converter(v_encoder1_input, encoder_vocab)\n",
    "v_encoder2_input = v_sequence_converter(v_encoder2_input, encoder_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import evaluate\n",
    "\n",
    "def validate_bleu(model1, v_encoder1_input, v_encoder2_input, decoder_input, decoder_target_lengths, epoch):\n",
    "    bi = []\n",
    "    for b in range(0,20,bs):\n",
    "        max_len = np.max(decoder_target_lengths[b:b+bs])\n",
    "        x = []\n",
    "        for a in decoder_target[b:b+bs]:\n",
    "            x.append((a + [1] * int((max_len - len(a)))))\n",
    "        x= np.array(x).T\n",
    "        feed_dict = {model1.encoder1_inputs: v_encoder1_input[:,b:b+bs],\n",
    "                             model1.encoder2_inputs: v_encoder2_input[:,b:b+bs],\n",
    "                             model1.decoder_inputs: decoder_input[:,b:b+bs],\n",
    "                             model1.decoder_input_length: decoder_target_lengths[b:b+bs],\n",
    "                             model1.decoder_outputs: x,\n",
    "                             model1.training: False\n",
    "                            }\n",
    "        bi.append(sess.run(model1.op, feed_dict=feed_dict))\n",
    "        \n",
    "    f = open('op.txt','w+')\n",
    "\n",
    "    for b in bi:\n",
    "        for i in range(len(b[:,:,0])):\n",
    "            for j in range(len(b[i,:,0])):\n",
    "                if b[i,j,0] == -1 or b[i,j,0] == 1:\n",
    "                    break\n",
    "                else:\n",
    "                    f.write(decoder_id2word[b[i,j,0]]+' ')\n",
    "            f.write('\\n')\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    f = open('op_epoch'+str(epoch)+'.txt','w+')\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for lines in open(\"op.txt\"):\n",
    "        if i < 3528:\n",
    "            f.write(lines.strip()+'\\n')\n",
    "            i=i+1\n",
    "        else:\n",
    "            break\n",
    "    f.close()\n",
    "    \n",
    "    evaluate('op_epoch'+str(epoch)+'.txt', 'WeatherGov/WeatherGov/dev/summaries.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(\n",
    "                     mode = \"train\",\n",
    "                     src_vocab_size=len(encoder_vocab), \n",
    "                     tgt_vocab_size=len(decoder_vocab), \n",
    "                     embedding_size=256, \n",
    "                     batch_size=bs, \n",
    "                     learning_rate = 0.001\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(\n",
    "                     mode = \"train\",\n",
    "                     src_vocab_size=len(encoder_vocab), \n",
    "                     tgt_vocab_size=len(decoder_vocab), \n",
    "                     embedding_size=256, \n",
    "                     batch_size=bs, \n",
    "                     learning_rate = 0.001\n",
    "                    )\n",
    "\n",
    "model1 = Seq2SeqModel(\n",
    "                     mode = \"train\",\n",
    "                     src_vocab_size=len(encoder_vocab), \n",
    "                     tgt_vocab_size=len(decoder_vocab), \n",
    "                     embedding_size=256, \n",
    "                     batch_size=bs, \n",
    "                     learning_rate = 0.001,\n",
    "                     beam_search = True,\n",
    "                     beam_width = 5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "sess = tf.Session(config = config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validate_bleu(model1, v_encoder1_input, v_encoder2_input, decoder_input, decoder_target_lengths, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "for e in range(epochs):\n",
    "    e_loss = 0\n",
    "    for b in range(0,20,bs):\n",
    "        max_len = np.max(decoder_target_lengths[b:b+bs])\n",
    "        x = []\n",
    "        for a in decoder_target[b:b+bs]:\n",
    "            x+=((a + [1] * int((max_len - len(a)))))\n",
    "        x= np.array(x).T\n",
    "        x = np.reshape(x,(x.shape[0],1))\n",
    "        feed_dict = {model.encoder1_inputs: encoder1_input[:,b:b+bs],\n",
    "                     model.encoder2_inputs: encoder2_input[:,b:b+bs],\n",
    "                     model.decoder_inputs: decoder_input[:,b:b+bs],\n",
    "                     model.decoder_input_length: decoder_target_lengths[b:b+bs],\n",
    "                     model.decoder_outputs: x,\n",
    "                     model.training: True\n",
    "                    }\n",
    "        a, b1, c = sess.run([model.logits,model.loss,model.train_op], feed_dict=feed_dict)\n",
    "        e_loss += b1\n",
    "    print(e_loss)\n",
    "    print(validate_bleu(model1, v_encoder1_input, v_encoder2_input, decoder_input, decoder_target_lengths, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "v_encoder1_input = v_readFile('WeatherGov/WeatherGov/test/test.proc')\n",
    "\n",
    "v_encoder2_input = v_readFile('WeatherGov/WeatherGov/test/test.field')\n",
    "\n",
    "v_encoder1_input = v_sequence_converter(v_encoder1_input, encoder_vocab)\n",
    "v_encoder2_input = v_sequence_converter(v_encoder2_input, encoder_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
