{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "#hyper-parameters\n",
    "INPUT_LAYER_SIZE=784\n",
    "OUTPUT_LAYER_SIZE=10\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('./data/train.csv')\n",
    "val=pd.read_csv('./data/val.csv')\n",
    "test=pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_class =train.iloc[:,-1].copy().as_matrix()\n",
    "training_data=sk.preprocessing.normalize(train.iloc[:,1:785].copy().as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validating_data_class =val.iloc[:,-1].copy().as_matrix()\n",
    "validating_data=sk.preprocessing.normalize(val.iloc[:,1:785].copy().as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(training_data, training_data_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_data=clf.transform(training_data)\n",
    "validating_data=clf.transform(validating_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_output(class_labels):\n",
    "    output=np.zeros((len(class_labels),10))\n",
    "    for i in range(len(class_labels)):\n",
    "        output[i,class_labels[i]]=1\n",
    "    return output\n",
    "def sigmoid(value):\n",
    "    return 1/(1+np.exp(-value))\n",
    "def tanh(value):\n",
    "    return np.tanh(value)\n",
    "def sigmoid_dif(value):\n",
    "    return sigmoid(value)*(1-sigmoid(value))\n",
    "def tanh_dif(value):\n",
    "    return 1-tanh(value)*tanh(value)\n",
    "def softmax(vector):\n",
    "    num= np.exp(vector)\n",
    "    return num/np.sum(num)\n",
    "def paraCopy(w,b,multiplier=1):\n",
    "    return [i*multiplier for i in w],[i*multiplier for i in b]\n",
    "def paraAdd(A,B):\n",
    "    a,b=A\n",
    "    c,d=B\n",
    "    return [i+j for i,j in zip(a,c)],[i+j for i,j in zip(b,d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shaper(lst):\n",
    "    for i in lst:\n",
    "        print 'shape -',i.shape\n",
    "def loss(list1,list2):\n",
    "    return -np.sum([j*np.log2(i) for i,j in zip(list1,list2)])\n",
    "def dumpModel(model):\n",
    "    kp=0\n",
    "    for i,j in zip(model.weights,model.biases):\n",
    "        np.save('./temp/weights_'+str(kp),i)\n",
    "        np.save('./temp/biases_'+str(kp),j)\n",
    "        kp+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN_Model:\n",
    "    def __init__(self,lr=0.01,momentum=0,hidden_layers=(100,),activation='sigmoid',loss='sq',\n",
    "                 opt='gd',batch_size=1,anneal=False,save_dir='./temp/',expt_dir='./temp/',max_iter=50):\n",
    "        self.lr=lr\n",
    "        self.momentum=momentum\n",
    "        self.layers=(INPUT_LAYER_SIZE,)+hidden_layers+(OUTPUT_LAYER_SIZE,)\n",
    "        if activation=='sigmoid':\n",
    "            self.activation=sigmoid\n",
    "        elif activation=='tanh':\n",
    "            self.activation=tanh\n",
    "        else:\n",
    "            print 'Error : activation function not found'\n",
    "                \n",
    "        \n",
    "        self.loss=loss\n",
    "        self.opt=opt\n",
    "        self.batch_size=batch_size\n",
    "        self.anneal=anneal\n",
    "        self.save_dir=save_dir\n",
    "        self.expt_dir=expt_dir\n",
    "        self.num_layers=len(self.layers)\n",
    "        self.max_iter=max_iter\n",
    "    \n",
    "    def __forward_propagation(self):\n",
    "        h_set=[]\n",
    "        a_set=[]\n",
    "        h=self.input_data[TRAINER]\n",
    "        h_set.append(h) #experimental\n",
    "        L=self.num_layers-2 # 2 are input and output layers\n",
    "        for k in range(L): \n",
    "            a=self.biases[k]+np.matmul(self.weights[k],h)\n",
    "            h=self.activation(a)\n",
    "            a_set.append(a)\n",
    "            h_set.append(h)\n",
    "        a=self.biases[L]+np.matmul(self.weights[L],h)\n",
    "        a_set.append(a)\n",
    "        y=softmax(a)\n",
    "        #h_set.append(y) #experimental\n",
    "        #print h.shape\n",
    "            \n",
    "        return h_set,a_set,y\n",
    "    \n",
    "    def __forward_propagation_test(self,h):\n",
    "        L=self.num_layers-2 # 2 are input and output layers\n",
    "        for k in range(L): \n",
    "            a=self.biases[k]+np.matmul(self.weights[k],h)\n",
    "            h=self.activation(a)\n",
    "        a=self.biases[L]+np.matmul(self.weights[L],h)\n",
    "        y=softmax(a)\n",
    "        #h_set.append(y) #experimental\n",
    "        #print h.shape\n",
    "            \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def __back_propagation(self,h,a,y):\n",
    "        L=self.num_layers-2\n",
    "        dWeights=[]\n",
    "        dBiases=[]\n",
    "        \n",
    "        daL_loss=-(self.output_data[TRAINER]-y) # for cross-entropy loss function\n",
    "        dA_loss=daL_loss\n",
    "        for k in range(L,-1,-1):\n",
    "            #print 'k=',k\n",
    "            dW_loss=np.outer(dA_loss,h[k]).T\n",
    "            dB_loss=dA_loss\n",
    "            \n",
    "            if k!=0:\n",
    "                dH_loss=np.matmul(self.weights[k].T,dA_loss)\n",
    "            \n",
    "                dA_loss=dH_loss*sigmoid_dif(a[k-1])\n",
    "            \n",
    "            \n",
    "            dWeights=[dW_loss.T]+dWeights\n",
    "            dBiases=[dB_loss.T]+dBiases\n",
    "            #print dA_loss.shape\n",
    "            #return _,_\n",
    "            \n",
    "            \n",
    "        return dWeights,dBiases\n",
    "    \n",
    "    def __update(self,dWeights,dBiases):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i]=self.weights[i]-dWeights[i]\n",
    "            self.biases[i]=self.biases[i]-dBiases[i]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __train(self):\n",
    "        global TRAINER\n",
    "        oldDWeights,oldDBiases=paraCopy(self.weights,self.biases,0) # for momentum\n",
    "            \n",
    "        for i in range(self.max_iter):\n",
    "            if i%5==0 and i!=0 and self.anneal==True:\n",
    "                self.lr/=2\n",
    "            \n",
    "            accuracy=0\n",
    "            if self.batch_size==1:\n",
    "                self.batch_size=len(training_data)\n",
    "            \n",
    "            sets=len(training_data)/self.batch_size # number of batches\n",
    "                \n",
    "            for j in range(sets):\n",
    "                #print 'SET=',j\n",
    "                los=0\n",
    "                dWeights,dBiases=paraCopy(self.weights,self.biases,0)\n",
    "                for TRAINER in range(j*self.batch_size,(j+1)*self.batch_size):\n",
    "                    #print 'TRAINER=',TRAINER\n",
    "                    h,a,y=self.__forward_propagation()\n",
    "\n",
    "                    \n",
    "                    dWeights,dBiases=paraAdd(self.__back_propagation(h,a,y),\n",
    "                                         paraCopy(dWeights,dBiases,1))\n",
    "                    los+=loss(y,self.output_data[TRAINER])\n",
    "\n",
    "                dWeights,dBiases=paraAdd(paraCopy(oldDWeights,oldDBiases,self.momentum),\n",
    "                                         paraCopy(dWeights,dBiases,self.lr))\n",
    "\n",
    "                self.__update(dWeights,dBiases)\n",
    "\n",
    "\n",
    "                \n",
    "                oldDWeights,oldDBiases=paraCopy(dWeights,dBiases,1)\n",
    "\n",
    "                #print np.argmax(y),self.raw_class_labels[TRAINER]\n",
    "                #if np.argmax(y)==self.raw_class_labels[TRAINER]:\n",
    "\n",
    "                 #   accuracy+=1\n",
    "                print 'Epoch : ',i,'Step : ',j,'loss : ',los\n",
    "                #print 'acc=',accuracy*1.0/sets\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        self.input_data=X\n",
    "        self.output_data=create_output(Y)\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        self.raw_class_labels=Y\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.random.uniform(low=-1.0,high=1.0,size=(self.layers[i+1],self.layers[i])))#-np.random.rand(self.layers[i+1],self.layers[i]))\n",
    "            self.biases.append(np.random.uniform(low=-1.0,high=1.0,size=(self.layers[i+1])))#-np.random.rand(self.layers[i+1]))\n",
    "            self.weights.append(np.random.uniform(self.layers[i+1],self.layers[i]))\n",
    "            self.biases.append(np.random.rand(self.layers[i+1]))\n",
    "            \n",
    "            #print self.biases[-1].shape\n",
    "        self.__train()\n",
    "        \n",
    "    def resume(self,X,Y):\n",
    "        self.input_data=X\n",
    "        self.output_data=create_output(Y)\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        self.raw_class_labels=Y\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.load('./temp/weights_'+str(i)+'.npy'))\n",
    "            self.biases.append(np.load('./temp/biases_'+str(i)+'.npy'))\n",
    "            #print self.weights[-1].shape\n",
    "        self.__train()\n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        return [np.argmax(self.__forward_propagation_test(i)) for i in X]    \n",
    "    def buildModel(self):\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.load('./temp/weights_'+str(i)+'.npy'))\n",
    "            self.biases.append(np.load('./temp/biases_'+str(i)+'.npy'))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=NN_Model(hidden_layers=(256,128),lr=0.001,max_iter=20,momentum=0.5,batch_size=250,anneal=True)\n",
    "training_data=np.array(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Scalar operands are not allowed, use '*' instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3d16884ba134>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_data_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#model.buildModel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a31a3f129e56>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m#print self.biases[-1].shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a31a3f129e56>\u001b[0m in \u001b[0;36m__train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mTRAINER\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0;31m#print 'TRAINER=',TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a31a3f129e56>\u001b[0m in \u001b[0;36m__forward_propagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# 2 are input and output layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0ma_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Scalar operands are not allowed, use '*' instead"
     ]
    }
   ],
   "source": [
    "model.fit(training_data,training_data_class)\n",
    "#model.buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8048"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(validating_data)-np.count_nonzero(model.predict(validating_data)-validating_data_class))/5000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dumpModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data=sk.preprocessing.normalize(test.iloc[:,1:785].copy().as_matrix())\n",
    "#testing_data=clf.transform(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_results=model.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp=[i for i in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_result=np.array(zip(tmp,test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    6]\n",
      " [   1    4]\n",
      " [   2    2]\n",
      " ..., \n",
      " [9997    3]\n",
      " [9998    7]\n",
      " [9999    6]]\n"
     ]
    }
   ],
   "source": [
    "print csv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('./res_LATEST.csv',csv_result,delimiter=',',fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
