{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import sys\n",
    "#hyper-parameters\n",
    "INPUT_LAYER_SIZE=9\n",
    "OUTPUT_LAYER_SIZE=10\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('./data/train.csv')\n",
    "val=pd.read_csv('./data/val.csv')\n",
    "test=pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_class =train.iloc[:,-1].copy().as_matrix()\n",
    "training_data=train.iloc[:,1:785].copy().as_matrix()/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validating_data_class =val.iloc[:,-1].copy().as_matrix()\n",
    "validating_data=val.iloc[:,1:785].copy().as_matrix()/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data=sk.preprocessing.scale(training_data)\n",
    "validating_data=sk.preprocessing.scale(validating_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07072376  0.10359759 -0.07072376 ..., -0.07072376 -0.07072376\n",
      "  0.10359759]\n"
     ]
    }
   ],
   "source": [
    "print (training_data[:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(training_data,training_data_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data=clf.transform(training_data)\n",
    "validating_data=clf.transform(validating_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data=sk.preprocessing.normalize(training_data)\n",
    "validating_data=sk.preprocessing.normalize(validating_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 9)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training_data=np.vstack((training_data,validating_data))\n",
    "#training_data_class=np.concatenate((training_data_class,validating_data_class))\n",
    "training_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_output(class_labels):\n",
    "    '''returns one-hot vectors'''\n",
    "    output=np.zeros((len(class_labels),10))\n",
    "    for i in range(len(class_labels)):\n",
    "        output[i,class_labels[i]]=1\n",
    "    return output\n",
    "def sigmoid(value):\n",
    "    return 1/(1+np.exp(-value))\n",
    "def tanh(value):\n",
    "    return np.tanh(value)\n",
    "def relu(value):\n",
    "    return [max(0,i) for i in value]\n",
    "def relu_dife(value):\n",
    "    if value>0:\n",
    "        return 1\n",
    "    return 0\n",
    "def relu_dif(value):\n",
    "    return [relu_dife(i) for i in value]\n",
    "def sigmoid_dif(value):\n",
    "    return sigmoid(value)*(1-sigmoid(value))\n",
    "def tanh_dif(value):\n",
    "    return 1-tanh(value)*tanh(value)\n",
    "def softmax(vector):\n",
    "    vector=np.array(vector,dtype=np.float64)\n",
    "    if sum(vector)!=sum(vector):\n",
    "        #print vector\n",
    "        sys.exit()\n",
    "    num= np.exp(vector)\n",
    "    \n",
    "    return num/np.sum(num)\n",
    "def paraCopy(w,b,multiplier=1):\n",
    "    return [i*multiplier for i in w],[i*multiplier for i in b]\n",
    "def paraAdd(A,B):\n",
    "    a,b=A\n",
    "    c,d=B\n",
    "    return [i+j for i,j in zip(a,c)],[i+j for i,j in zip(b,d)]\n",
    "def cliper(A):\n",
    "    return A/np.linalg.norm(A)\n",
    "#def cliper(A,B):\n",
    "#    return [i/np.linalg.norm(i) for i in A],[j/np.linalg.norm(j) for j in B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shaper(lst):\n",
    "    for i in lst:\n",
    "        print 'shape -',i.shape\n",
    "def loss(list1,list2,loss_type):\n",
    "    if loss_type=='ce': #ce \n",
    "        return -np.sum([j*np.log2(i) for i,j in zip(list1,list2)])\n",
    "    if loss_type=='sq': #sq\n",
    "        return 0.5*np.sum([(i-j)**2 for i,j in zip(list1,list2)])\n",
    "def dumpModel(model):\n",
    "    kp=0\n",
    "    for i,j in zip(model.weights,model.biases):\n",
    "        np.save('./temp/weights_'+str(kp),i)\n",
    "        np.save('./temp/biases_'+str(kp),j)\n",
    "        kp+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN_Model:\n",
    "    def __init__(self,lr=0.01,momentum=0,hidden_layers=(100,),activation='sigmoid',loss='sq',\n",
    "                 opt='gd',batch_size=1,anneal=False,save_dir='./temp/',expt_dir='./temp/',max_iter=50,ae_mode=False,\n",
    "                noise=0.0,Lambda=0):\n",
    "        self.lr=lr\n",
    "        self.momentum=momentum\n",
    "        self.layers=(INPUT_LAYER_SIZE,)+hidden_layers+(OUTPUT_LAYER_SIZE,)\n",
    "        if activation=='sigmoid':\n",
    "            self.activation=sigmoid\n",
    "            self.activation_diff=sigmoid_dif\n",
    "        elif activation=='tanh':\n",
    "            self.activation=tanh\n",
    "            self.activation_diff=tanh_dif\n",
    "        elif activation=='relu':\n",
    "            self.activation=relu\n",
    "            self.activation_diff=relu_dif\n",
    "        else:\n",
    "            print 'Error : activation function not found'\n",
    "        self.activation_name=activation        \n",
    "        self.p_noise=noise\n",
    "        self.loss=loss\n",
    "        self.opt=opt\n",
    "        self.batch_size=batch_size\n",
    "        self.anneal=anneal\n",
    "        self.save_dir=save_dir\n",
    "        self.expt_dir=expt_dir\n",
    "        self.num_layers=len(self.layers)\n",
    "        self.max_iter=max_iter\n",
    "        self.ae_mode=ae_mode\n",
    "        self.lam=Lambda\n",
    "    \n",
    "    def __forward_propagation(self):\n",
    "        h_set=[]\n",
    "        a_set=[]\n",
    "        h=self.input_data[TRAINER]\n",
    "        h=h+np.random.normal(size=h.shape)*np.random.choice([0, 1], size=h.shape, p=[1-self.p_noise,self.p_noise])\n",
    "\n",
    "        h_set.append(h) #experimental\n",
    "        L=self.num_layers-2 # 2 are input and output layers\n",
    "        for k in range(L): \n",
    "            a=self.biases[k]+np.matmul(self.weights[k],h)\n",
    "            \n",
    "            h=self.activation(a)\n",
    "            if self.activation_name=='relu':\n",
    "                h=cliper(h)\n",
    "            a_set.append(a)\n",
    "            h_set.append(h)\n",
    "        a=self.biases[L]+np.matmul(self.weights[L],h)\n",
    "        \n",
    "        a_set.append(a)\n",
    "        #print a\n",
    "        y=softmax(a)\n",
    "        #sys.exit()\n",
    "        #h_set.append(y) #experimental\n",
    "        #print h.shape\n",
    "            \n",
    "        return h_set,a_set,y\n",
    "    \n",
    "    def __forward_propagation_test(self,h):\n",
    "        L=self.num_layers-2 # 2 are input and output layers\n",
    "        for k in range(L): \n",
    "            a=self.biases[k]+np.matmul(self.weights[k],h)\n",
    "            h=self.activation(a)\n",
    "            if self.activation_name=='relu':\n",
    "                h=cliper(h)\n",
    "            \n",
    "        a=self.biases[L]+np.matmul(self.weights[L],h)\n",
    "        if self.ae_mode==False:\n",
    "            y=softmax(a)\n",
    "        else:\n",
    "            y=a\n",
    "        #h_set.append(y) #experimental\n",
    "        #print h.shape\n",
    "            \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def __back_propagation(self,h,a,y):\n",
    "        L=self.num_layers-2\n",
    "        dWeights=[]\n",
    "        dBiases=[]\n",
    "        if self.loss=='ce':\n",
    "            daL_loss=-(self.output_data[TRAINER]-y) # for cross-entropy loss function\n",
    "        elif self.loss=='sq':\n",
    "            daL_loss=np.array([2*sum([(y[i]-self.output_data[TRAINER][i])*y[i]*((i==j)*1-y[j]) for i in range(len(y))]) for j in range(len(y))])\n",
    "        else :\n",
    "            print 'Wrong loss function'\n",
    "            return\n",
    "        dA_loss=daL_loss\n",
    "        for k in range(L,-1,-1):\n",
    "            #print 'k=',k\n",
    "            dW_loss=np.outer(dA_loss,h[k]).T\n",
    "            dB_loss=dA_loss\n",
    "            \n",
    "            if k!=0:\n",
    "                dH_loss=np.matmul(self.weights[k].T,dA_loss)\n",
    "            \n",
    "                dA_loss=dH_loss*self.activation_diff(a[k-1])\n",
    "            \n",
    "            \n",
    "            dWeights=[dW_loss.T]+dWeights\n",
    "            dBiases=[dB_loss.T]+dBiases\n",
    "            #print dA_loss.shape\n",
    "            #return _,_\n",
    "            \n",
    "            \n",
    "        return dWeights,dBiases\n",
    "    \n",
    "    def __update(self,dWeights,dBiases):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i]=self.weights[i]-(dWeights[i]+self.lam*np.absolute(self.weights[i]))\n",
    "            self.biases[i]=self.biases[i]-(dBiases[i]+self.lam*np.absolute(self.biases[i]))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __train(self):\n",
    "        global TRAINER\n",
    "        for i in range(self.max_iter):\n",
    "            if i%5==0 and i!=0 and self.anneal==True:\n",
    "                self.lr/=2\n",
    "            \n",
    "            accuracy=0\n",
    "            if self.batch_size==1:\n",
    "                self.batch_size=len(training_data)\n",
    "            \n",
    "            sets=len(training_data)/self.batch_size # number of batches\n",
    "                \n",
    "            oldDWeights,oldDBiases=paraCopy(self.weights,self.biases,0) # for momentum\n",
    "            for j in range(sets):\n",
    "                #print 'SET=',j\n",
    "                los=0\n",
    "                dWeights,dBiases=paraCopy(self.weights,self.biases,0)\n",
    "                for TRAINER in range(j*self.batch_size,(j+1)*self.batch_size):\n",
    "                    #print 'TRAINER=',TRAINER\n",
    "                    h,a,y=self.__forward_propagation()\n",
    "\n",
    "                    \n",
    "                    dWeights,dBiases=paraAdd(self.__back_propagation(h,a,y),\n",
    "                                         paraCopy(dWeights,dBiases,1))\n",
    "                    \n",
    "                    los+=loss(y,self.output_data[TRAINER],self.loss)\n",
    "\n",
    "                dWeights,dBiases=paraAdd(paraCopy(oldDWeights,oldDBiases,self.momentum),\n",
    "                                         paraCopy(dWeights,dBiases,self.lr))\n",
    "\n",
    "                self.__update(dWeights,dBiases)\n",
    "\n",
    "\n",
    "                \n",
    "                oldDWeights,oldDBiases=paraCopy(dWeights,dBiases,1)\n",
    "\n",
    "                #print np.argmax(y),self.raw_class_labels[TRAINER]\n",
    "                #if np.argmax(y)==self.raw_class_labels[TRAINER]:\n",
    "\n",
    "                 #   accuracy+=1\n",
    "                print 'Epoch : ',i,'Step : ',j,'loss : ',los\n",
    "                #print 'acc=',accuracy*1.0/sets\n",
    "            #print 'validation loss ',sum([loss(i,)])\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        self.input_data=X\n",
    "        if self.ae_mode== False:\n",
    "            self.output_data=create_output(Y)\n",
    "        else:\n",
    "            self.output_data=Y\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        self.raw_class_labels=Y\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.random.uniform(low=-1.0,high=1.0,size=(self.layers[i+1],self.layers[i])))\n",
    "            self.biases.append(np.random.uniform(low=-1.0,high=1.0,size=(self.layers[i+1])))\n",
    "            \n",
    "            #print self.biases[-1].shape\n",
    "        self.__train()\n",
    "        \n",
    "    def resume(self,X,Y):\n",
    "        self.input_data=X\n",
    "        self.output_data=create_output(Y)\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        self.raw_class_labels=Y\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.load('./temp/weights_'+str(i)+'.npy'))\n",
    "            self.biases.append(np.load('./temp/biases_'+str(i)+'.npy'))\n",
    "            #print self.weights[-1].shape\n",
    "        self.__train()\n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        return [np.argmax(self.__forward_propagation_test(i)) for i in X]    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=NN_Model(hidden_layers=(9,9),lr=0.001,max_iter=20,momentum=0.3,batch_size=250,anneal=True,loss='ce',\n",
    "               activation='sigmoid',ae_mode=False,noise=0.1,Lambda=0.0)\n",
    "training_data=np.array(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0 Step :  0 loss :  949.568996195\n",
      "Epoch :  0 Step :  1 loss :  962.277718067\n",
      "Epoch :  0 Step :  2 loss :  966.956076574\n",
      "Epoch :  0 Step :  3 loss :  903.876371101\n",
      "Epoch :  0 Step :  4 loss :  932.259154086\n",
      "Epoch :  0 Step :  5 loss :  881.754058334\n",
      "Epoch :  0 Step :  6 loss :  889.162099628\n",
      "Epoch :  0 Step :  7 loss :  872.807238052\n",
      "Epoch :  0 Step :  8 loss :  855.102647925\n",
      "Epoch :  0 Step :  9 loss :  837.871131576\n",
      "Epoch :  0 Step :  10 loss :  853.19296026\n",
      "Epoch :  0 Step :  11 loss :  847.576830157\n",
      "Epoch :  0 Step :  12 loss :  842.940664151\n",
      "Epoch :  0 Step :  13 loss :  843.339473672\n",
      "Epoch :  0 Step :  14 loss :  846.505212755\n",
      "Epoch :  0 Step :  15 loss :  838.442644072\n",
      "Epoch :  0 Step :  16 loss :  830.831582699\n",
      "Epoch :  0 Step :  17 loss :  841.7254581\n",
      "Epoch :  0 Step :  18 loss :  832.688800661\n",
      "Epoch :  0 Step :  19 loss :  829.154419468\n",
      "Epoch :  0 Step :  20 loss :  828.235112548\n",
      "Epoch :  0 Step :  21 loss :  832.675099823\n",
      "Epoch :  0 Step :  22 loss :  829.675227195\n",
      "Epoch :  0 Step :  23 loss :  825.661537232\n",
      "Epoch :  0 Step :  24 loss :  825.175512476\n",
      "Epoch :  0 Step :  25 loss :  825.133023105\n",
      "Epoch :  0 Step :  26 loss :  827.708724872\n",
      "Epoch :  0 Step :  27 loss :  827.771057446\n",
      "Epoch :  0 Step :  28 loss :  825.923307813\n",
      "Epoch :  0 Step :  29 loss :  825.755148774\n",
      "Epoch :  0 Step :  30 loss :  825.596228444\n",
      "Epoch :  0 Step :  31 loss :  828.128815326\n",
      "Epoch :  0 Step :  32 loss :  823.684320333\n",
      "Epoch :  0 Step :  33 loss :  822.404100132\n",
      "Epoch :  0 Step :  34 loss :  826.391448892\n",
      "Epoch :  0 Step :  35 loss :  825.450616371\n",
      "Epoch :  0 Step :  36 loss :  824.284388231\n",
      "Epoch :  0 Step :  37 loss :  822.484125894\n",
      "Epoch :  0 Step :  38 loss :  824.299325425\n",
      "Epoch :  0 Step :  39 loss :  822.01223024\n",
      "Epoch :  0 Step :  40 loss :  820.65539794\n",
      "Epoch :  0 Step :  41 loss :  821.628953316\n",
      "Epoch :  0 Step :  42 loss :  820.372358362\n",
      "Epoch :  0 Step :  43 loss :  820.873254196\n",
      "Epoch :  0 Step :  44 loss :  822.604446992\n",
      "Epoch :  0 Step :  45 loss :  823.266419797\n",
      "Epoch :  0 Step :  46 loss :  819.664183178\n",
      "Epoch :  0 Step :  47 loss :  821.766696334\n",
      "Epoch :  0 Step :  48 loss :  820.916138887\n",
      "Epoch :  0 Step :  49 loss :  821.220287119\n",
      "Epoch :  0 Step :  50 loss :  824.63375849\n",
      "Epoch :  0 Step :  51 loss :  818.771279189\n",
      "Epoch :  0 Step :  52 loss :  819.042487107\n",
      "Epoch :  0 Step :  53 loss :  813.681933117\n",
      "Epoch :  0 Step :  54 loss :  817.318980057\n",
      "Epoch :  0 Step :  55 loss :  818.608862691\n",
      "Epoch :  0 Step :  56 loss :  817.490728167\n",
      "Epoch :  0 Step :  57 loss :  815.020327792\n",
      "Epoch :  0 Step :  58 loss :  818.959249962\n",
      "Epoch :  0 Step :  59 loss :  816.982876836\n",
      "Epoch :  0 Step :  60 loss :  812.719462313\n",
      "Epoch :  0 Step :  61 loss :  817.501821537\n",
      "Epoch :  0 Step :  62 loss :  812.08976065\n",
      "Epoch :  0 Step :  63 loss :  819.826798254\n",
      "Epoch :  0 Step :  64 loss :  812.55978642\n",
      "Epoch :  0 Step :  65 loss :  807.137339889\n",
      "Epoch :  0 Step :  66 loss :  812.567214467\n",
      "Epoch :  0 Step :  67 loss :  818.250749121\n",
      "Epoch :  0 Step :  68 loss :  812.988596849\n",
      "Epoch :  0 Step :  69 loss :  812.733586987\n",
      "Epoch :  0 Step :  70 loss :  812.975939359\n",
      "Epoch :  0 Step :  71 loss :  813.468937931\n",
      "Epoch :  0 Step :  72 loss :  814.285596962\n",
      "Epoch :  0 Step :  73 loss :  815.312342624\n",
      "Epoch :  0 Step :  74 loss :  807.431053272\n",
      "Epoch :  0 Step :  75 loss :  809.849592259\n",
      "Epoch :  0 Step :  76 loss :  813.754565558\n",
      "Epoch :  0 Step :  77 loss :  811.083054643\n",
      "Epoch :  0 Step :  78 loss :  813.081665715\n",
      "Epoch :  0 Step :  79 loss :  810.28141889\n",
      "Epoch :  0 Step :  80 loss :  808.686439159\n",
      "Epoch :  0 Step :  81 loss :  808.532584812\n",
      "Epoch :  0 Step :  82 loss :  811.041999702\n",
      "Epoch :  0 Step :  83 loss :  810.791638588\n",
      "Epoch :  0 Step :  84 loss :  807.200974428\n",
      "Epoch :  0 Step :  85 loss :  807.976847526\n",
      "Epoch :  0 Step :  86 loss :  805.807005092\n",
      "Epoch :  0 Step :  87 loss :  803.133205562\n",
      "Epoch :  0 Step :  88 loss :  804.886128411\n",
      "Epoch :  0 Step :  89 loss :  804.983627124\n",
      "Epoch :  0 Step :  90 loss :  802.254743904\n",
      "Epoch :  0 Step :  91 loss :  802.105459315\n",
      "Epoch :  0 Step :  92 loss :  800.96253978\n",
      "Epoch :  0 Step :  93 loss :  810.92803162\n",
      "Epoch :  0 Step :  94 loss :  797.420684298\n",
      "Epoch :  0 Step :  95 loss :  802.28926248\n",
      "Epoch :  0 Step :  96 loss :  801.813276048\n",
      "Epoch :  0 Step :  97 loss :  798.720134938\n",
      "Epoch :  0 Step :  98 loss :  803.099574229\n",
      "Epoch :  0 Step :  99 loss :  809.351645382\n",
      "Epoch :  0 Step :  100 loss :  802.286940225\n",
      "Epoch :  0 Step :  101 loss :  802.580062401\n",
      "Epoch :  0 Step :  102 loss :  806.991853929\n",
      "Epoch :  0 Step :  103 loss :  800.602769033\n",
      "Epoch :  0 Step :  104 loss :  804.548076297\n",
      "Epoch :  0 Step :  105 loss :  802.686541399\n",
      "Epoch :  0 Step :  106 loss :  799.374123853\n",
      "Epoch :  0 Step :  107 loss :  794.128397999\n",
      "Epoch :  0 Step :  108 loss :  799.983904636\n",
      "Epoch :  0 Step :  109 loss :  798.270222785\n",
      "Epoch :  0 Step :  110 loss :  797.981024676\n",
      "Epoch :  0 Step :  111 loss :  793.696651061\n",
      "Epoch :  0 Step :  112 loss :  795.230174359\n",
      "Epoch :  0 Step :  113 loss :  797.318252749\n",
      "Epoch :  0 Step :  114 loss :  792.393480027\n",
      "Epoch :  0 Step :  115 loss :  797.280472961\n",
      "Epoch :  0 Step :  116 loss :  795.083858398\n",
      "Epoch :  0 Step :  117 loss :  796.193240043\n",
      "Epoch :  0 Step :  118 loss :  792.539672576\n",
      "Epoch :  0 Step :  119 loss :  787.606098432\n",
      "Epoch :  0 Step :  120 loss :  787.306439816\n",
      "Epoch :  0 Step :  121 loss :  789.152145539\n",
      "Epoch :  0 Step :  122 loss :  790.003309216\n",
      "Epoch :  0 Step :  123 loss :  795.113504814\n",
      "Epoch :  0 Step :  124 loss :  786.895760047\n",
      "Epoch :  0 Step :  125 loss :  784.182004059\n",
      "Epoch :  0 Step :  126 loss :  790.145899933\n",
      "Epoch :  0 Step :  127 loss :  789.670462169\n",
      "Epoch :  0 Step :  128 loss :  791.898866275\n",
      "Epoch :  0 Step :  129 loss :  785.420628223\n",
      "Epoch :  0 Step :  130 loss :  785.666045451\n",
      "Epoch :  0 Step :  131 loss :  795.055137425\n",
      "Epoch :  0 Step :  132 loss :  784.536282369\n",
      "Epoch :  0 Step :  133 loss :  784.347126508\n",
      "Epoch :  0 Step :  134 loss :  789.707614553\n",
      "Epoch :  0 Step :  135 loss :  781.424443783\n",
      "Epoch :  0 Step :  136 loss :  784.565773762\n",
      "Epoch :  0 Step :  137 loss :  788.155384194\n",
      "Epoch :  0 Step :  138 loss :  787.254674174\n",
      "Epoch :  0 Step :  139 loss :  785.160262536\n",
      "Epoch :  0 Step :  140 loss :  785.173682539\n",
      "Epoch :  0 Step :  141 loss :  778.63874486\n",
      "Epoch :  0 Step :  142 loss :  785.126590378\n",
      "Epoch :  0 Step :  143 loss :  775.865418596\n",
      "Epoch :  0 Step :  144 loss :  777.802235903\n",
      "Epoch :  0 Step :  145 loss :  790.501395929\n",
      "Epoch :  0 Step :  146 loss :  781.628720643\n",
      "Epoch :  0 Step :  147 loss :  789.81991908\n",
      "Epoch :  0 Step :  148 loss :  780.293368248\n",
      "Epoch :  0 Step :  149 loss :  784.374264061\n",
      "Epoch :  0 Step :  150 loss :  776.885672114\n",
      "Epoch :  0 Step :  151 loss :  773.36838132\n",
      "Epoch :  0 Step :  152 loss :  778.475519687\n",
      "Epoch :  0 Step :  153 loss :  773.410299915\n",
      "Epoch :  0 Step :  154 loss :  772.218084825\n",
      "Epoch :  0 Step :  155 loss :  770.612326557\n",
      "Epoch :  0 Step :  156 loss :  775.37212406\n",
      "Epoch :  0 Step :  157 loss :  768.522536152\n",
      "Epoch :  0 Step :  158 loss :  781.53565144\n",
      "Epoch :  0 Step :  159 loss :  770.074466662\n",
      "Epoch :  0 Step :  160 loss :  766.508691877\n",
      "Epoch :  0 Step :  161 loss :  776.188259691\n",
      "Epoch :  0 Step :  162 loss :  778.785787259\n",
      "Epoch :  0 Step :  163 loss :  764.305625692\n",
      "Epoch :  0 Step :  164 loss :  772.161036287\n",
      "Epoch :  0 Step :  165 loss :  765.781047265\n",
      "Epoch :  0 Step :  166 loss :  772.145191796\n",
      "Epoch :  0 Step :  167 loss :  769.129328876\n",
      "Epoch :  0 Step :  168 loss :  774.249836295\n",
      "Epoch :  0 Step :  169 loss :  765.24860079\n",
      "Epoch :  0 Step :  170 loss :  763.218585514\n",
      "Epoch :  0 Step :  171 loss :  769.020612361\n",
      "Epoch :  0 Step :  172 loss :  759.669332669\n",
      "Epoch :  0 Step :  173 loss :  770.356748128\n",
      "Epoch :  0 Step :  174 loss :  756.784207969\n",
      "Epoch :  0 Step :  175 loss :  763.776060219\n",
      "Epoch :  0 Step :  176 loss :  760.756061357\n",
      "Epoch :  0 Step :  177 loss :  766.738429093\n",
      "Epoch :  0 Step :  178 loss :  753.066580257\n",
      "Epoch :  0 Step :  179 loss :  762.714107749\n",
      "Epoch :  0 Step :  180 loss :  747.973913611\n",
      "Epoch :  0 Step :  181 loss :  754.364692904\n",
      "Epoch :  0 Step :  182 loss :  752.943755657\n",
      "Epoch :  0 Step :  183 loss :  752.06833532\n",
      "Epoch :  0 Step :  184 loss :  753.115137586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0 Step :  185 loss :  758.818727828\n",
      "Epoch :  0 Step :  186 loss :  755.194152279\n",
      "Epoch :  0 Step :  187 loss :  749.722464594\n",
      "Epoch :  0 Step :  188 loss :  750.300828687\n",
      "Epoch :  0 Step :  189 loss :  759.728056793\n",
      "Epoch :  0 Step :  190 loss :  745.950743494\n",
      "Epoch :  0 Step :  191 loss :  761.670942934\n",
      "Epoch :  0 Step :  192 loss :  738.470129352\n",
      "Epoch :  0 Step :  193 loss :  729.798446815\n",
      "Epoch :  0 Step :  194 loss :  752.479320402\n",
      "Epoch :  0 Step :  195 loss :  745.170985555\n",
      "Epoch :  0 Step :  196 loss :  738.809407662\n",
      "Epoch :  0 Step :  197 loss :  741.528505155\n",
      "Epoch :  0 Step :  198 loss :  726.821094069\n",
      "Epoch :  0 Step :  199 loss :  749.156213039\n",
      "Epoch :  0 Step :  200 loss :  745.092164395\n",
      "Epoch :  0 Step :  201 loss :  737.029015474\n",
      "Epoch :  0 Step :  202 loss :  736.044000882\n",
      "Epoch :  0 Step :  203 loss :  732.027825354\n",
      "Epoch :  0 Step :  204 loss :  744.340463791\n",
      "Epoch :  0 Step :  205 loss :  737.204681766\n",
      "Epoch :  0 Step :  206 loss :  752.789417247\n",
      "Epoch :  0 Step :  207 loss :  746.290101189\n",
      "Epoch :  0 Step :  208 loss :  723.875781522\n",
      "Epoch :  0 Step :  209 loss :  728.553701233\n",
      "Epoch :  0 Step :  210 loss :  742.877193776\n",
      "Epoch :  0 Step :  211 loss :  743.610203138\n",
      "Epoch :  0 Step :  212 loss :  727.267743293\n",
      "Epoch :  0 Step :  213 loss :  727.708514223\n",
      "Epoch :  0 Step :  214 loss :  740.027545144\n",
      "Epoch :  0 Step :  215 loss :  736.713584558\n",
      "Epoch :  0 Step :  216 loss :  735.560183509\n",
      "Epoch :  0 Step :  217 loss :  726.002817925\n",
      "Epoch :  0 Step :  218 loss :  737.947149501\n",
      "Epoch :  0 Step :  219 loss :  720.706762823\n",
      "Epoch :  1 Step :  0 loss :  716.010450905\n",
      "Epoch :  1 Step :  1 loss :  731.37832563\n",
      "Epoch :  1 Step :  2 loss :  725.340105647\n",
      "Epoch :  1 Step :  3 loss :  720.96125041\n",
      "Epoch :  1 Step :  4 loss :  713.001172668\n",
      "Epoch :  1 Step :  5 loss :  725.193415102\n",
      "Epoch :  1 Step :  6 loss :  710.979218006\n",
      "Epoch :  1 Step :  7 loss :  708.898057411\n",
      "Epoch :  1 Step :  8 loss :  714.893015099\n",
      "Epoch :  1 Step :  9 loss :  721.490537364\n",
      "Epoch :  1 Step :  10 loss :  722.314169614\n",
      "Epoch :  1 Step :  11 loss :  713.601741415\n",
      "Epoch :  1 Step :  12 loss :  701.651472828\n",
      "Epoch :  1 Step :  13 loss :  708.915027628\n",
      "Epoch :  1 Step :  14 loss :  709.644689015\n",
      "Epoch :  1 Step :  15 loss :  709.028607113\n",
      "Epoch :  1 Step :  16 loss :  701.325432126\n",
      "Epoch :  1 Step :  17 loss :  708.366493636\n",
      "Epoch :  1 Step :  18 loss :  691.067843967\n",
      "Epoch :  1 Step :  19 loss :  704.941612897\n",
      "Epoch :  1 Step :  20 loss :  710.366498903\n",
      "Epoch :  1 Step :  21 loss :  720.109777313\n",
      "Epoch :  1 Step :  22 loss :  683.066240677\n",
      "Epoch :  1 Step :  23 loss :  693.612893414\n",
      "Epoch :  1 Step :  24 loss :  709.664209253\n",
      "Epoch :  1 Step :  25 loss :  692.920905018\n",
      "Epoch :  1 Step :  26 loss :  698.594234738\n",
      "Epoch :  1 Step :  27 loss :  702.616139168\n",
      "Epoch :  1 Step :  28 loss :  690.151400043\n",
      "Epoch :  1 Step :  29 loss :  692.118887347\n",
      "Epoch :  1 Step :  30 loss :  708.987829303\n",
      "Epoch :  1 Step :  31 loss :  694.550669794\n",
      "Epoch :  1 Step :  32 loss :  698.369667157\n",
      "Epoch :  1 Step :  33 loss :  689.333522414\n",
      "Epoch :  1 Step :  34 loss :  696.567803126\n",
      "Epoch :  1 Step :  35 loss :  698.492979091\n",
      "Epoch :  1 Step :  36 loss :  681.896757039\n",
      "Epoch :  1 Step :  37 loss :  688.43342455\n",
      "Epoch :  1 Step :  38 loss :  686.003794261\n",
      "Epoch :  1 Step :  39 loss :  693.8237758\n",
      "Epoch :  1 Step :  40 loss :  678.616989446\n",
      "Epoch :  1 Step :  41 loss :  688.893421756\n",
      "Epoch :  1 Step :  42 loss :  685.111934694\n",
      "Epoch :  1 Step :  43 loss :  686.53253379\n",
      "Epoch :  1 Step :  44 loss :  680.786099761\n",
      "Epoch :  1 Step :  45 loss :  684.768931633\n",
      "Epoch :  1 Step :  46 loss :  677.599372441\n",
      "Epoch :  1 Step :  47 loss :  689.333642696\n",
      "Epoch :  1 Step :  48 loss :  686.379470285\n",
      "Epoch :  1 Step :  49 loss :  669.512946807\n",
      "Epoch :  1 Step :  50 loss :  671.292892233\n",
      "Epoch :  1 Step :  51 loss :  667.587362197\n",
      "Epoch :  1 Step :  52 loss :  658.209175968\n",
      "Epoch :  1 Step :  53 loss :  652.78470836\n",
      "Epoch :  1 Step :  54 loss :  665.470085268\n",
      "Epoch :  1 Step :  55 loss :  670.378909572\n",
      "Epoch :  1 Step :  56 loss :  676.497921475\n",
      "Epoch :  1 Step :  57 loss :  673.180773754\n",
      "Epoch :  1 Step :  58 loss :  664.85233951\n",
      "Epoch :  1 Step :  59 loss :  676.298621706\n",
      "Epoch :  1 Step :  60 loss :  663.546944316\n",
      "Epoch :  1 Step :  61 loss :  661.619187563\n",
      "Epoch :  1 Step :  62 loss :  654.597548347\n",
      "Epoch :  1 Step :  63 loss :  660.882568357\n",
      "Epoch :  1 Step :  64 loss :  648.906311216\n",
      "Epoch :  1 Step :  65 loss :  638.521430723\n",
      "Epoch :  1 Step :  66 loss :  647.029614448\n",
      "Epoch :  1 Step :  67 loss :  664.227024486\n",
      "Epoch :  1 Step :  68 loss :  656.781048617\n",
      "Epoch :  1 Step :  69 loss :  649.164649764\n",
      "Epoch :  1 Step :  70 loss :  648.698691647\n",
      "Epoch :  1 Step :  71 loss :  661.838768162\n",
      "Epoch :  1 Step :  72 loss :  659.194630749\n",
      "Epoch :  1 Step :  73 loss :  649.996497913\n",
      "Epoch :  1 Step :  74 loss :  638.721371795\n",
      "Epoch :  1 Step :  75 loss :  633.900231105\n",
      "Epoch :  1 Step :  76 loss :  657.745646875\n",
      "Epoch :  1 Step :  77 loss :  652.512489362\n",
      "Epoch :  1 Step :  78 loss :  649.339801146\n",
      "Epoch :  1 Step :  79 loss :  646.185116055\n",
      "Epoch :  1 Step :  80 loss :  635.704180851\n",
      "Epoch :  1 Step :  81 loss :  633.357385683\n",
      "Epoch :  1 Step :  82 loss :  634.407902116\n",
      "Epoch :  1 Step :  83 loss :  634.151501945\n",
      "Epoch :  1 Step :  84 loss :  626.125561176\n",
      "Epoch :  1 Step :  85 loss :  634.805460588\n",
      "Epoch :  1 Step :  86 loss :  634.370690572\n",
      "Epoch :  1 Step :  87 loss :  626.291805993\n",
      "Epoch :  1 Step :  88 loss :  637.295901321\n",
      "Epoch :  1 Step :  89 loss :  623.002512396\n",
      "Epoch :  1 Step :  90 loss :  622.265795339\n",
      "Epoch :  1 Step :  91 loss :  637.556764492\n",
      "Epoch :  1 Step :  92 loss :  618.624028313\n",
      "Epoch :  1 Step :  93 loss :  626.868031939\n",
      "Epoch :  1 Step :  94 loss :  616.266817264\n",
      "Epoch :  1 Step :  95 loss :  630.058970109\n",
      "Epoch :  1 Step :  96 loss :  613.493209063\n",
      "Epoch :  1 Step :  97 loss :  622.846362042\n",
      "Epoch :  1 Step :  98 loss :  638.559090548\n",
      "Epoch :  1 Step :  99 loss :  642.965847971\n",
      "Epoch :  1 Step :  100 loss :  622.490767249\n",
      "Epoch :  1 Step :  101 loss :  622.057576542\n",
      "Epoch :  1 Step :  102 loss :  614.414990154\n",
      "Epoch :  1 Step :  103 loss :  616.323208467\n",
      "Epoch :  1 Step :  104 loss :  636.910969213\n",
      "Epoch :  1 Step :  105 loss :  611.655278908\n",
      "Epoch :  1 Step :  106 loss :  612.6479661\n",
      "Epoch :  1 Step :  107 loss :  602.891859861\n",
      "Epoch :  1 Step :  108 loss :  615.212162283\n",
      "Epoch :  1 Step :  109 loss :  609.850510044\n",
      "Epoch :  1 Step :  110 loss :  611.533691173\n",
      "Epoch :  1 Step :  111 loss :  613.045339036\n",
      "Epoch :  1 Step :  112 loss :  600.16699045\n",
      "Epoch :  1 Step :  113 loss :  606.917874577\n",
      "Epoch :  1 Step :  114 loss :  589.259881637\n",
      "Epoch :  1 Step :  115 loss :  598.02573035\n",
      "Epoch :  1 Step :  116 loss :  595.216291402\n",
      "Epoch :  1 Step :  117 loss :  601.700363274\n",
      "Epoch :  1 Step :  118 loss :  611.611156869\n",
      "Epoch :  1 Step :  119 loss :  581.685043834\n",
      "Epoch :  1 Step :  120 loss :  592.915583835\n",
      "Epoch :  1 Step :  121 loss :  599.060727852\n",
      "Epoch :  1 Step :  122 loss :  598.367673709\n",
      "Epoch :  1 Step :  123 loss :  602.293883839\n",
      "Epoch :  1 Step :  124 loss :  578.175652114\n",
      "Epoch :  1 Step :  125 loss :  593.892282592\n",
      "Epoch :  1 Step :  126 loss :  596.691176069\n",
      "Epoch :  1 Step :  127 loss :  602.998597431\n",
      "Epoch :  1 Step :  128 loss :  591.697209621\n",
      "Epoch :  1 Step :  129 loss :  587.124514821\n",
      "Epoch :  1 Step :  130 loss :  560.005825205\n",
      "Epoch :  1 Step :  131 loss :  604.342987544\n",
      "Epoch :  1 Step :  132 loss :  589.911024438\n",
      "Epoch :  1 Step :  133 loss :  572.359778538\n",
      "Epoch :  1 Step :  134 loss :  600.081081584\n",
      "Epoch :  1 Step :  135 loss :  606.027573363\n",
      "Epoch :  1 Step :  136 loss :  568.708821946\n",
      "Epoch :  1 Step :  137 loss :  599.372933623\n",
      "Epoch :  1 Step :  138 loss :  603.632669328\n",
      "Epoch :  1 Step :  139 loss :  580.910096259\n",
      "Epoch :  1 Step :  140 loss :  576.961338287\n",
      "Epoch :  1 Step :  141 loss :  588.66051671\n",
      "Epoch :  1 Step :  142 loss :  586.200152084\n",
      "Epoch :  1 Step :  143 loss :  571.793887327\n",
      "Epoch :  1 Step :  144 loss :  587.165865458\n",
      "Epoch :  1 Step :  145 loss :  596.631027179\n",
      "Epoch :  1 Step :  146 loss :  590.238422452\n",
      "Epoch :  1 Step :  147 loss :  599.438987204\n",
      "Epoch :  1 Step :  148 loss :  578.316050707\n",
      "Epoch :  1 Step :  149 loss :  575.245607682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 Step :  150 loss :  574.09395606\n",
      "Epoch :  1 Step :  151 loss :  576.494810488\n",
      "Epoch :  1 Step :  152 loss :  595.184410616\n",
      "Epoch :  1 Step :  153 loss :  546.07864104\n",
      "Epoch :  1 Step :  154 loss :  558.251001323\n",
      "Epoch :  1 Step :  155 loss :  569.688336115\n",
      "Epoch :  1 Step :  156 loss :  576.671205065\n",
      "Epoch :  1 Step :  157 loss :  557.042029655\n",
      "Epoch :  1 Step :  158 loss :  585.735369942\n",
      "Epoch :  1 Step :  159 loss :  555.727231625\n",
      "Epoch :  1 Step :  160 loss :  560.865004287\n",
      "Epoch :  1 Step :  161 loss :  571.544858216\n",
      "Epoch :  1 Step :  162 loss :  557.694602705\n",
      "Epoch :  1 Step :  163 loss :  551.803107356\n",
      "Epoch :  1 Step :  164 loss :  563.727712347\n",
      "Epoch :  1 Step :  165 loss :  555.026678575\n",
      "Epoch :  1 Step :  166 loss :  546.29664524\n",
      "Epoch :  1 Step :  167 loss :  542.431958029\n",
      "Epoch :  1 Step :  168 loss :  585.104044755\n",
      "Epoch :  1 Step :  169 loss :  562.402338759\n",
      "Epoch :  1 Step :  170 loss :  558.508238269\n",
      "Epoch :  1 Step :  171 loss :  566.668806974\n",
      "Epoch :  1 Step :  172 loss :  547.531020905\n",
      "Epoch :  1 Step :  173 loss :  566.704042817\n",
      "Epoch :  1 Step :  174 loss :  545.211577201\n",
      "Epoch :  1 Step :  175 loss :  562.071536136\n",
      "Epoch :  1 Step :  176 loss :  542.706297909\n",
      "Epoch :  1 Step :  177 loss :  580.217876705\n",
      "Epoch :  1 Step :  178 loss :  541.903422269\n",
      "Epoch :  1 Step :  179 loss :  562.898968368\n",
      "Epoch :  1 Step :  180 loss :  530.914198242\n",
      "Epoch :  1 Step :  181 loss :  547.488013152\n",
      "Epoch :  1 Step :  182 loss :  538.786140626\n",
      "Epoch :  1 Step :  183 loss :  539.714360973\n",
      "Epoch :  1 Step :  184 loss :  537.67822613\n",
      "Epoch :  1 Step :  185 loss :  560.603121487\n",
      "Epoch :  1 Step :  186 loss :  551.199271425\n",
      "Epoch :  1 Step :  187 loss :  535.114401935\n",
      "Epoch :  1 Step :  188 loss :  546.497659766\n",
      "Epoch :  1 Step :  189 loss :  542.2537105\n",
      "Epoch :  1 Step :  190 loss :  554.835327559\n",
      "Epoch :  1 Step :  191 loss :  540.805832787\n",
      "Epoch :  1 Step :  192 loss :  511.810650904\n",
      "Epoch :  1 Step :  193 loss :  525.91585787\n",
      "Epoch :  1 Step :  194 loss :  545.939598413\n",
      "Epoch :  1 Step :  195 loss :  547.499071379\n",
      "Epoch :  1 Step :  196 loss :  528.294111713\n",
      "Epoch :  1 Step :  197 loss :  509.541214807\n",
      "Epoch :  1 Step :  198 loss :  498.774828341\n",
      "Epoch :  1 Step :  199 loss :  536.217917429\n",
      "Epoch :  1 Step :  200 loss :  537.6225303\n",
      "Epoch :  1 Step :  201 loss :  550.51202442\n",
      "Epoch :  1 Step :  202 loss :  538.247226064\n",
      "Epoch :  1 Step :  203 loss :  519.444524334\n",
      "Epoch :  1 Step :  204 loss :  546.233364134\n",
      "Epoch :  1 Step :  205 loss :  528.786322213\n",
      "Epoch :  1 Step :  206 loss :  537.766768497\n",
      "Epoch :  1 Step :  207 loss :  555.61277935\n",
      "Epoch :  1 Step :  208 loss :  521.40693624\n",
      "Epoch :  1 Step :  209 loss :  524.973844271\n",
      "Epoch :  1 Step :  210 loss :  557.554734264\n",
      "Epoch :  1 Step :  211 loss :  538.326828573\n",
      "Epoch :  1 Step :  212 loss :  522.839247861\n",
      "Epoch :  1 Step :  213 loss :  520.009715552\n",
      "Epoch :  1 Step :  214 loss :  545.734572901\n",
      "Epoch :  1 Step :  215 loss :  526.980943781\n",
      "Epoch :  1 Step :  216 loss :  531.317515931\n",
      "Epoch :  1 Step :  217 loss :  535.587744243\n",
      "Epoch :  1 Step :  218 loss :  535.833751529\n",
      "Epoch :  1 Step :  219 loss :  542.215657299\n",
      "Epoch :  2 Step :  0 loss :  533.963540851\n",
      "Epoch :  2 Step :  1 loss :  546.259438256\n",
      "Epoch :  2 Step :  2 loss :  512.560335189\n",
      "Epoch :  2 Step :  3 loss :  525.107323842\n",
      "Epoch :  2 Step :  4 loss :  516.882009134\n",
      "Epoch :  2 Step :  5 loss :  548.26210519\n",
      "Epoch :  2 Step :  6 loss :  525.670180439\n",
      "Epoch :  2 Step :  7 loss :  495.221996846\n",
      "Epoch :  2 Step :  8 loss :  523.370061085\n",
      "Epoch :  2 Step :  9 loss :  517.430803278\n",
      "Epoch :  2 Step :  10 loss :  536.755835505\n",
      "Epoch :  2 Step :  11 loss :  512.295540877\n",
      "Epoch :  2 Step :  12 loss :  527.669628105\n",
      "Epoch :  2 Step :  13 loss :  508.781910351\n",
      "Epoch :  2 Step :  14 loss :  526.147385968\n",
      "Epoch :  2 Step :  15 loss :  512.881990081\n",
      "Epoch :  2 Step :  16 loss :  502.384252217\n",
      "Epoch :  2 Step :  17 loss :  534.451152903\n",
      "Epoch :  2 Step :  18 loss :  502.831245725\n",
      "Epoch :  2 Step :  19 loss :  508.649271983\n",
      "Epoch :  2 Step :  20 loss :  484.693669306\n",
      "Epoch :  2 Step :  21 loss :  554.449817328\n",
      "Epoch :  2 Step :  22 loss :  500.273341875\n",
      "Epoch :  2 Step :  23 loss :  512.79416123\n",
      "Epoch :  2 Step :  24 loss :  506.220156648\n",
      "Epoch :  2 Step :  25 loss :  508.202757228\n",
      "Epoch :  2 Step :  26 loss :  499.067023523\n",
      "Epoch :  2 Step :  27 loss :  475.905407424\n",
      "Epoch :  2 Step :  28 loss :  507.547476226\n",
      "Epoch :  2 Step :  29 loss :  514.665021362\n",
      "Epoch :  2 Step :  30 loss :  541.664011471\n",
      "Epoch :  2 Step :  31 loss :  523.987413548\n",
      "Epoch :  2 Step :  32 loss :  517.294838734\n",
      "Epoch :  2 Step :  33 loss :  513.047041703\n",
      "Epoch :  2 Step :  34 loss :  507.716023812\n",
      "Epoch :  2 Step :  35 loss :  514.750652944\n",
      "Epoch :  2 Step :  36 loss :  493.193802129\n",
      "Epoch :  2 Step :  37 loss :  496.078251849\n",
      "Epoch :  2 Step :  38 loss :  492.587519795\n",
      "Epoch :  2 Step :  39 loss :  526.940547952\n",
      "Epoch :  2 Step :  40 loss :  497.492727083\n",
      "Epoch :  2 Step :  41 loss :  490.980146437\n",
      "Epoch :  2 Step :  42 loss :  501.780984886\n",
      "Epoch :  2 Step :  43 loss :  496.848213727\n",
      "Epoch :  2 Step :  44 loss :  493.753748799\n",
      "Epoch :  2 Step :  45 loss :  506.703312884\n",
      "Epoch :  2 Step :  46 loss :  515.862410711\n",
      "Epoch :  2 Step :  47 loss :  539.637344765\n",
      "Epoch :  2 Step :  48 loss :  509.783204835\n",
      "Epoch :  2 Step :  49 loss :  502.700314009\n",
      "Epoch :  2 Step :  50 loss :  486.38874594\n",
      "Epoch :  2 Step :  51 loss :  474.338044662\n",
      "Epoch :  2 Step :  52 loss :  460.033017428\n",
      "Epoch :  2 Step :  53 loss :  485.717087377\n",
      "Epoch :  2 Step :  54 loss :  488.806171093\n",
      "Epoch :  2 Step :  55 loss :  515.713746569\n",
      "Epoch :  2 Step :  56 loss :  498.427439925\n",
      "Epoch :  2 Step :  57 loss :  522.165616596\n",
      "Epoch :  2 Step :  58 loss :  502.815261751\n",
      "Epoch :  2 Step :  59 loss :  533.196195416\n",
      "Epoch :  2 Step :  60 loss :  484.898195949\n",
      "Epoch :  2 Step :  61 loss :  509.726165769\n",
      "Epoch :  2 Step :  62 loss :  518.677527684\n",
      "Epoch :  2 Step :  63 loss :  479.519828633\n",
      "Epoch :  2 Step :  64 loss :  486.44795174\n",
      "Epoch :  2 Step :  65 loss :  473.817804908\n",
      "Epoch :  2 Step :  66 loss :  513.325890743\n",
      "Epoch :  2 Step :  67 loss :  490.432330032\n",
      "Epoch :  2 Step :  68 loss :  493.138137923\n",
      "Epoch :  2 Step :  69 loss :  501.841982575\n",
      "Epoch :  2 Step :  70 loss :  492.967409789\n",
      "Epoch :  2 Step :  71 loss :  517.971041433\n",
      "Epoch :  2 Step :  72 loss :  510.614700528\n",
      "Epoch :  2 Step :  73 loss :  483.825386497\n",
      "Epoch :  2 Step :  74 loss :  448.321729126\n",
      "Epoch :  2 Step :  75 loss :  482.843069144\n",
      "Epoch :  2 Step :  76 loss :  479.571604724\n",
      "Epoch :  2 Step :  77 loss :  479.276593161\n",
      "Epoch :  2 Step :  78 loss :  467.69849618\n",
      "Epoch :  2 Step :  79 loss :  492.224649532\n",
      "Epoch :  2 Step :  80 loss :  505.76861226\n",
      "Epoch :  2 Step :  81 loss :  457.32683814\n",
      "Epoch :  2 Step :  82 loss :  478.821191485\n",
      "Epoch :  2 Step :  83 loss :  469.519053847\n",
      "Epoch :  2 Step :  84 loss :  477.867563617\n",
      "Epoch :  2 Step :  85 loss :  491.426995562\n",
      "Epoch :  2 Step :  86 loss :  474.599154995\n",
      "Epoch :  2 Step :  87 loss :  509.436815495\n",
      "Epoch :  2 Step :  88 loss :  496.220191837\n",
      "Epoch :  2 Step :  89 loss :  508.954996594\n",
      "Epoch :  2 Step :  90 loss :  522.256251087\n",
      "Epoch :  2 Step :  91 loss :  521.423122363\n",
      "Epoch :  2 Step :  92 loss :  491.274210813\n",
      "Epoch :  2 Step :  93 loss :  491.660934092\n",
      "Epoch :  2 Step :  94 loss :  495.309916113\n",
      "Epoch :  2 Step :  95 loss :  445.308926438\n",
      "Epoch :  2 Step :  96 loss :  471.422748301\n",
      "Epoch :  2 Step :  97 loss :  500.200288696\n",
      "Epoch :  2 Step :  98 loss :  500.620599127\n",
      "Epoch :  2 Step :  99 loss :  503.786657898\n",
      "Epoch :  2 Step :  100 loss :  474.915953308\n",
      "Epoch :  2 Step :  101 loss :  486.50813086\n",
      "Epoch :  2 Step :  102 loss :  470.186500722\n",
      "Epoch :  2 Step :  103 loss :  455.215035352\n",
      "Epoch :  2 Step :  104 loss :  478.651194563\n",
      "Epoch :  2 Step :  105 loss :  476.786603565\n",
      "Epoch :  2 Step :  106 loss :  453.505585515\n",
      "Epoch :  2 Step :  107 loss :  452.715460907\n",
      "Epoch :  2 Step :  108 loss :  483.083540771\n",
      "Epoch :  2 Step :  109 loss :  469.157006049\n",
      "Epoch :  2 Step :  110 loss :  452.534651962\n",
      "Epoch :  2 Step :  111 loss :  483.29860972\n",
      "Epoch :  2 Step :  112 loss :  472.375433559\n",
      "Epoch :  2 Step :  113 loss :  465.936817861\n",
      "Epoch :  2 Step :  114 loss :  465.771033945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  2 Step :  115 loss :  456.505682685\n",
      "Epoch :  2 Step :  116 loss :  483.448346314\n",
      "Epoch :  2 Step :  117 loss :  506.897118361\n",
      "Epoch :  2 Step :  118 loss :  488.985402225\n",
      "Epoch :  2 Step :  119 loss :  452.885548481\n",
      "Epoch :  2 Step :  120 loss :  474.460445133\n",
      "Epoch :  2 Step :  121 loss :  448.374798375\n",
      "Epoch :  2 Step :  122 loss :  454.64655842\n",
      "Epoch :  2 Step :  123 loss :  451.261472417\n",
      "Epoch :  2 Step :  124 loss :  457.108048017\n",
      "Epoch :  2 Step :  125 loss :  468.48330196\n",
      "Epoch :  2 Step :  126 loss :  452.809430002\n",
      "Epoch :  2 Step :  127 loss :  465.209884776\n",
      "Epoch :  2 Step :  128 loss :  465.810996485\n",
      "Epoch :  2 Step :  129 loss :  451.428044599\n",
      "Epoch :  2 Step :  130 loss :  448.034854022\n",
      "Epoch :  2 Step :  131 loss :  471.330751584\n",
      "Epoch :  2 Step :  132 loss :  458.971982011\n",
      "Epoch :  2 Step :  133 loss :  479.298717579\n",
      "Epoch :  2 Step :  134 loss :  461.292817073\n",
      "Epoch :  2 Step :  135 loss :  437.432167967\n",
      "Epoch :  2 Step :  136 loss :  451.016671615\n",
      "Epoch :  2 Step :  137 loss :  484.84342077\n",
      "Epoch :  2 Step :  138 loss :  460.089612671\n",
      "Epoch :  2 Step :  139 loss :  444.163599792\n",
      "Epoch :  2 Step :  140 loss :  485.977920048\n",
      "Epoch :  2 Step :  141 loss :  453.650455846\n",
      "Epoch :  2 Step :  142 loss :  482.045664697\n",
      "Epoch :  2 Step :  143 loss :  465.147353706\n",
      "Epoch :  2 Step :  144 loss :  458.981066451\n",
      "Epoch :  2 Step :  145 loss :  477.052816484\n",
      "Epoch :  2 Step :  146 loss :  469.610778264\n",
      "Epoch :  2 Step :  147 loss :  454.175509385\n",
      "Epoch :  2 Step :  148 loss :  456.136132928\n"
     ]
    }
   ],
   "source": [
    "model.fit(training_data,training_data_class)\n",
    "#model.resume(training_data,training_data_class)\n",
    "#model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6144"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(validating_data)-np.count_nonzero(model.predict(validating_data)-validating_data_class))/5000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dumpModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data=test.iloc[:,1:785].copy().as_matrix()/255.0\n",
    "#testing_data=clf.transform(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_results=model.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp=[i for i in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_result=np.array(zip(tmp,test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    4]\n",
      " [   1    4]\n",
      " [   2    2]\n",
      " ..., \n",
      " [9997    3]\n",
      " [9998    7]\n",
      " [9999    3]]\n"
     ]
    }
   ],
   "source": [
    "print csv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('./res.csv',csv_result,delimiter=',',fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
