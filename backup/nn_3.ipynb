{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "#hyper-parameters\n",
    "INPUT_LAYER_SIZE=784\n",
    "OUTPUT_LAYER_SIZE=10\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('./data/train.csv')\n",
    "val=pd.read_csv('./data/val.csv')\n",
    "test=pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_class =train.iloc[:,-1].copy().as_matrix()\n",
    "training_data=(train.iloc[:,1:785].copy().as_matrix())/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validating_data_class =val.iloc[:,-1].copy().as_matrix()\n",
    "validating_data=(val.iloc[:,1:785].copy().as_matrix())/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mak/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(training_data, training_data_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data=clf.transform(training_data)\n",
    "validating_data=clf.transform(validating_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_output(class_labels):\n",
    "    output=np.zeros((len(class_labels),10))\n",
    "    for i in range(len(class_labels)):\n",
    "        output[i,class_labels[i]]=1\n",
    "    return output\n",
    "def sigmoid(value):\n",
    "    return 1/(1+np.exp(-value))\n",
    "def tanh(value):\n",
    "    return np.tanh(value)\n",
    "def sigmoid_dif(value):\n",
    "    return sigmoid(value)*(1-sigmoid(value))\n",
    "def tanh_dif(value):\n",
    "    return 1-tanh(value)*tanh(value)\n",
    "def softmax(vector):\n",
    "    num= np.exp(vector)\n",
    "    return num/np.sum(num)\n",
    "def paraCopy(w,b,multiplier=1):\n",
    "    return [i*multiplier for i in w],[i*multiplier for i in b]\n",
    "def paraAdd(A,B):\n",
    "    a,b=A\n",
    "    c,d=B\n",
    "    return [i+j for i,j in zip(a,c)],[i+j for i,j in zip(b,d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shaper(lst):\n",
    "    for i in lst:\n",
    "        print 'shape -',i.shape\n",
    "def loss(list1,list2):\n",
    "    return -np.sum([j*np.log2(i) for i,j in zip(list1,list2)])\n",
    "def dumpModel(model):\n",
    "    kp=0\n",
    "    for i,j in zip(model.weights,model.biases):\n",
    "        np.save('./temp/weights_'+str(kp),i)\n",
    "        np.save('./temp/biases_'+str(kp),j)\n",
    "        kp+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Model:\n",
    "    def __init__(self,lr=0.01,momentum=0,hidden_layers=(100,),activation='sigmoid',loss='ce',\n",
    "                 opt='gd',batch_size=1,anneal=False,save_dir='./temp/',expt_dir='./temp/',max_iter=50):\n",
    "        self.lr=lr\n",
    "        self.momentum=momentum\n",
    "        self.layers=(INPUT_LAYER_SIZE,)+hidden_layers+(OUTPUT_LAYER_SIZE,)\n",
    "        if activation=='sigmoid':\n",
    "            self.activation=sigmoid\n",
    "            self.activation_dif=sigmoid_dif\n",
    "        elif activation=='tanh':\n",
    "            self.activation=tanh\n",
    "            self.activation_dif=tanh_dif\n",
    "        else:\n",
    "            print 'Error : activation function not found'\n",
    "                \n",
    "        \n",
    "        self.loss=loss\n",
    "        self.opt=opt\n",
    "        self.batch_size=batch_size\n",
    "        self.anneal=anneal\n",
    "        self.save_dir=save_dir\n",
    "        self.expt_dir=expt_dir\n",
    "        self.num_layers=len(self.layers)\n",
    "        self.max_iter=max_iter\n",
    "    \n",
    "    def __forward_propagation(self):\n",
    "        h_set=[]\n",
    "        a_set=[]\n",
    "        h=self.input_data[TRAINER]\n",
    "        h_set.append(h) #experimental\n",
    "        L=self.num_layers-2 # 2 are input and output layers\n",
    "        for k in range(L): \n",
    "            a=self.biases[k]+np.matmul(self.weights[k],h)\n",
    "            h=self.activation(a)\n",
    "            a_set.append(a)\n",
    "            h_set.append(h)\n",
    "        a=self.biases[L]+np.matmul(self.weights[L],h)\n",
    "        a_set.append(a)\n",
    "        y=softmax(a)\n",
    "        #h_set.append(y) #experimental\n",
    "        #print h.shape\n",
    "            \n",
    "        return h_set,a_set,y\n",
    "    \n",
    "    def __forward_propagation_test(self,h):\n",
    "        L=self.num_layers-2 # 2 are input and output layers\n",
    "        for k in range(L): \n",
    "            a=self.biases[k]+np.matmul(self.weights[k],h)\n",
    "            h=self.activation(a)\n",
    "        a=self.biases[L]+np.matmul(self.weights[L],h)\n",
    "        y=softmax(a)\n",
    "        #h_set.append(y) #experimental\n",
    "        #print h.shape\n",
    "            \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def __back_propagation(self,h,a,y):\n",
    "        L=self.num_layers-2\n",
    "        dWeights=[]\n",
    "        dBiases=[]\n",
    "        \n",
    "        daL_loss=-(self.output_data[TRAINER]-y) # for cross-entropy loss function\n",
    "        dA_loss=daL_loss\n",
    "        for k in range(L,-1,-1):\n",
    "            #print 'k=',k\n",
    "            dW_loss=np.outer(dA_loss,h[k]).T\n",
    "            dB_loss=dA_loss\n",
    "            \n",
    "            if k!=0:\n",
    "                dH_loss=np.matmul(self.weights[k].T,dA_loss)\n",
    "            \n",
    "                dA_loss=dH_loss*self.activation_dif(a[k-1])\n",
    "            \n",
    "            \n",
    "            dWeights=[dW_loss.T]+dWeights\n",
    "            dBiases=[dB_loss.T]+dBiases\n",
    "            #print dA_loss.shape\n",
    "            #return _,_\n",
    "            \n",
    "            \n",
    "        return dWeights,dBiases\n",
    "    \n",
    "    def __update(self,dWeights,dBiases):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i]=self.weights[i]-dWeights[i]\n",
    "            self.biases[i]=self.biases[i]-dBiases[i]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __train(self):\n",
    "        global TRAINER\n",
    "        if self.batch_size==1:\n",
    "            self.batch_size=len(self.input_data)\n",
    "            \n",
    "        sets=len(self.input_data)/self.batch_size # number of batches\n",
    "            \n",
    "        for i in range(self.max_iter):\n",
    "            np.random.shuffle(self.input_data)\n",
    "            accuracy=0\n",
    "            if i%5==0 and i!=0 and self.anneal==True:\n",
    "                self.lr/=2\n",
    "            oldDWeights,oldDBiases=paraCopy(self.weights,self.biases,0) # for momentum\n",
    "            for j in range(sets):\n",
    "                #print 'SET=',j\n",
    "                los=0\n",
    "                dWeights,dBiases=paraCopy(self.weights,self.biases,0)\n",
    "                for TRAINER in range(j*self.batch_size,(j+1)*self.batch_size):\n",
    "                    #print 'TRAINER=',TRAINER\n",
    "                    h,a,y=self.__forward_propagation()\n",
    "\n",
    "                    \n",
    "                    dWeights,dBiases=paraAdd(self.__back_propagation(h,a,y),\n",
    "                                         paraCopy(dWeights,dBiases,1))\n",
    "                    los+=loss(y,self.output_data[TRAINER])\n",
    "\n",
    "                dWeights,dBiases=paraAdd(paraCopy(oldDWeights,oldDBiases,self.momentum),\n",
    "                                         paraCopy(dWeights,dBiases,self.lr))\n",
    "\n",
    "                self.__update(dWeights,dBiases)\n",
    "\n",
    "\n",
    "                \n",
    "                oldDWeights,oldDBiases=paraCopy(dWeights,dBiases,1)\n",
    "\n",
    "                #print np.argmax(y),self.raw_class_labels[TRAINER]\n",
    "                #if np.argmax(y)==self.raw_class_labels[TRAINER]:\n",
    "\n",
    "                 #   accuracy+=1\n",
    "                print '<Epoch : ',i,'><Step : ',j,'><loss : ',los,'>'\n",
    "                #print 'acc=',accuracy*1.0/sets\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        self.input_data=X\n",
    "        self.output_data=create_output(Y)\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        self.raw_class_labels=Y\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.random.rand(self.layers[i+1],self.layers[i])-np.random.rand(self.layers[i+1],self.layers[i]))\n",
    "            self.biases.append(np.random.rand(self.layers[i+1])-np.random.rand(self.layers[i+1]))\n",
    "            #print self.biases[-1].shape\n",
    "        self.__train()\n",
    "        \n",
    "    def resume(self,X,Y):\n",
    "        self.input_data=X\n",
    "        self.output_data=create_output(Y)\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        self.raw_class_labels=Y\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.load('./temp/weights_'+str(i)+'.npy'))\n",
    "            self.biases.append(np.load('./temp/biases_'+str(i)+'.npy'))\n",
    "            #print self.weights[-1].shape\n",
    "        self.__train()\n",
    "    def buildModel(self):\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.load('./temp/weights_'+str(i)+'.npy'))\n",
    "            self.biases.append(np.load('./temp/biases_'+str(i)+'.npy'))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        return [np.argmax(self.__forward_propagation_test(i)) for i in X]    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NN_Model(hidden_layers=(30,),lr=0.001,max_iter=5,momentum=0.5,batch_size=25,anneal=False)\n",
    "training_data=np.array(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Epoch :  0 ><Step :  0 ><loss :  97.5859634385 >\n",
      "<Epoch :  0 ><Step :  1 ><loss :  93.2920822606 >\n",
      "<Epoch :  0 ><Step :  2 ><loss :  94.2219365718 >\n",
      "<Epoch :  0 ><Step :  3 ><loss :  93.0983735973 >\n",
      "<Epoch :  0 ><Step :  4 ><loss :  91.9387732884 >\n",
      "<Epoch :  0 ><Step :  5 ><loss :  106.29899839 >\n",
      "<Epoch :  0 ><Step :  6 ><loss :  96.5929476454 >\n",
      "<Epoch :  0 ><Step :  7 ><loss :  103.056165735 >\n",
      "<Epoch :  0 ><Step :  8 ><loss :  85.8642174542 >\n",
      "<Epoch :  0 ><Step :  9 ><loss :  94.2617756075 >\n",
      "<Epoch :  0 ><Step :  10 ><loss :  94.9324864696 >\n",
      "<Epoch :  0 ><Step :  11 ><loss :  99.5645282213 >\n",
      "<Epoch :  0 ><Step :  12 ><loss :  84.1574547801 >\n",
      "<Epoch :  0 ><Step :  13 ><loss :  86.8802620596 >\n",
      "<Epoch :  0 ><Step :  14 ><loss :  90.486321653 >\n",
      "<Epoch :  0 ><Step :  15 ><loss :  94.5591231271 >\n",
      "<Epoch :  0 ><Step :  16 ><loss :  91.2146859608 >\n",
      "<Epoch :  0 ><Step :  17 ><loss :  94.2532097926 >\n",
      "<Epoch :  0 ><Step :  18 ><loss :  82.3882604778 >\n",
      "<Epoch :  0 ><Step :  19 ><loss :  86.1908812173 >\n",
      "<Epoch :  0 ><Step :  20 ><loss :  83.7293697809 >\n",
      "<Epoch :  0 ><Step :  21 ><loss :  94.469043135 >\n",
      "<Epoch :  0 ><Step :  22 ><loss :  93.5431059906 >\n",
      "<Epoch :  0 ><Step :  23 ><loss :  79.3833730219 >\n",
      "<Epoch :  0 ><Step :  24 ><loss :  89.2047165732 >\n",
      "<Epoch :  0 ><Step :  25 ><loss :  84.3800822477 >\n",
      "<Epoch :  0 ><Step :  26 ><loss :  82.8240738764 >\n",
      "<Epoch :  0 ><Step :  27 ><loss :  88.7476583122 >\n",
      "<Epoch :  0 ><Step :  28 ><loss :  82.9418752684 >\n",
      "<Epoch :  0 ><Step :  29 ><loss :  86.8791135251 >\n",
      "<Epoch :  0 ><Step :  30 ><loss :  90.6949766655 >\n",
      "<Epoch :  0 ><Step :  31 ><loss :  83.4342717128 >\n",
      "<Epoch :  0 ><Step :  32 ><loss :  84.2017170252 >\n",
      "<Epoch :  0 ><Step :  33 ><loss :  92.7609161023 >\n",
      "<Epoch :  0 ><Step :  34 ><loss :  85.5596724198 >\n",
      "<Epoch :  0 ><Step :  35 ><loss :  83.1570464629 >\n",
      "<Epoch :  0 ><Step :  36 ><loss :  83.1090222276 >\n",
      "<Epoch :  0 ><Step :  37 ><loss :  86.0187437209 >\n",
      "<Epoch :  0 ><Step :  38 ><loss :  81.6600862154 >\n",
      "<Epoch :  0 ><Step :  39 ><loss :  84.4951745509 >\n",
      "<Epoch :  0 ><Step :  40 ><loss :  83.7035044748 >\n",
      "<Epoch :  0 ><Step :  41 ><loss :  89.8259208631 >\n",
      "<Epoch :  0 ><Step :  42 ><loss :  86.9984654275 >\n",
      "<Epoch :  0 ><Step :  43 ><loss :  82.7353097863 >\n",
      "<Epoch :  0 ><Step :  44 ><loss :  87.6893315266 >\n",
      "<Epoch :  0 ><Step :  45 ><loss :  81.9151227996 >\n",
      "<Epoch :  0 ><Step :  46 ><loss :  83.5619457369 >\n",
      "<Epoch :  0 ><Step :  47 ><loss :  84.9881836251 >\n",
      "<Epoch :  0 ><Step :  48 ><loss :  86.769457751 >\n",
      "<Epoch :  0 ><Step :  49 ><loss :  88.7638954351 >\n",
      "<Epoch :  0 ><Step :  50 ><loss :  88.8853520076 >\n",
      "<Epoch :  0 ><Step :  51 ><loss :  89.611420309 >\n",
      "<Epoch :  0 ><Step :  52 ><loss :  90.7216329182 >\n",
      "<Epoch :  0 ><Step :  53 ><loss :  87.1800125722 >\n",
      "<Epoch :  0 ><Step :  54 ><loss :  86.6657243776 >\n",
      "<Epoch :  0 ><Step :  55 ><loss :  87.8635185968 >\n",
      "<Epoch :  0 ><Step :  56 ><loss :  85.992438322 >\n",
      "<Epoch :  0 ><Step :  57 ><loss :  79.2797510546 >\n",
      "<Epoch :  0 ><Step :  58 ><loss :  81.9872389293 >\n",
      "<Epoch :  0 ><Step :  59 ><loss :  83.6602349179 >\n",
      "<Epoch :  0 ><Step :  60 ><loss :  85.4374511021 >\n",
      "<Epoch :  0 ><Step :  61 ><loss :  83.4918876682 >\n",
      "<Epoch :  0 ><Step :  62 ><loss :  80.8683262274 >\n",
      "<Epoch :  0 ><Step :  63 ><loss :  88.5057740437 >\n",
      "<Epoch :  0 ><Step :  64 ><loss :  87.2599472536 >\n",
      "<Epoch :  0 ><Step :  65 ><loss :  87.2142144343 >\n",
      "<Epoch :  0 ><Step :  66 ><loss :  81.135668145 >\n",
      "<Epoch :  0 ><Step :  67 ><loss :  87.3234319801 >\n",
      "<Epoch :  0 ><Step :  68 ><loss :  81.1749220767 >\n",
      "<Epoch :  0 ><Step :  69 ><loss :  85.0958461802 >\n",
      "<Epoch :  0 ><Step :  70 ><loss :  88.9622786895 >\n",
      "<Epoch :  0 ><Step :  71 ><loss :  83.5446300643 >\n",
      "<Epoch :  0 ><Step :  72 ><loss :  84.2727729361 >\n",
      "<Epoch :  0 ><Step :  73 ><loss :  83.4618291352 >\n",
      "<Epoch :  0 ><Step :  74 ><loss :  84.8520489218 >\n",
      "<Epoch :  0 ><Step :  75 ><loss :  89.566065346 >\n",
      "<Epoch :  0 ><Step :  76 ><loss :  87.9754283514 >\n",
      "<Epoch :  0 ><Step :  77 ><loss :  88.9008612524 >\n",
      "<Epoch :  0 ><Step :  78 ><loss :  88.0721282608 >\n",
      "<Epoch :  0 ><Step :  79 ><loss :  84.793278754 >\n",
      "<Epoch :  0 ><Step :  80 ><loss :  80.1060035995 >\n",
      "<Epoch :  0 ><Step :  81 ><loss :  84.309616769 >\n",
      "<Epoch :  0 ><Step :  82 ><loss :  88.1292344639 >\n",
      "<Epoch :  0 ><Step :  83 ><loss :  86.1509374893 >\n",
      "<Epoch :  0 ><Step :  84 ><loss :  85.2930120253 >\n",
      "<Epoch :  0 ><Step :  85 ><loss :  84.908108159 >\n",
      "<Epoch :  0 ><Step :  86 ><loss :  87.2450681426 >\n",
      "<Epoch :  0 ><Step :  87 ><loss :  84.1650712452 >\n",
      "<Epoch :  0 ><Step :  88 ><loss :  86.3085768108 >\n",
      "<Epoch :  0 ><Step :  89 ><loss :  84.3940583097 >\n",
      "<Epoch :  0 ><Step :  90 ><loss :  83.6407889768 >\n",
      "<Epoch :  0 ><Step :  91 ><loss :  91.7293312344 >\n",
      "<Epoch :  0 ><Step :  92 ><loss :  87.9861296529 >\n",
      "<Epoch :  0 ><Step :  93 ><loss :  84.2198587082 >\n",
      "<Epoch :  0 ><Step :  94 ><loss :  86.7249509545 >\n",
      "<Epoch :  0 ><Step :  95 ><loss :  88.0040958474 >\n",
      "<Epoch :  0 ><Step :  96 ><loss :  84.6308298578 >\n",
      "<Epoch :  0 ><Step :  97 ><loss :  85.4461367126 >\n",
      "<Epoch :  0 ><Step :  98 ><loss :  86.60963577 >\n",
      "<Epoch :  0 ><Step :  99 ><loss :  90.1912443387 >\n",
      "<Epoch :  0 ><Step :  100 ><loss :  87.3000375468 >\n",
      "<Epoch :  0 ><Step :  101 ><loss :  83.7558607474 >\n",
      "<Epoch :  0 ><Step :  102 ><loss :  83.4028856104 >\n",
      "<Epoch :  0 ><Step :  103 ><loss :  80.2517034971 >\n",
      "<Epoch :  0 ><Step :  104 ><loss :  85.7001251174 >\n",
      "<Epoch :  0 ><Step :  105 ><loss :  84.7504817116 >\n",
      "<Epoch :  0 ><Step :  106 ><loss :  87.0177800253 >\n",
      "<Epoch :  0 ><Step :  107 ><loss :  82.2587681178 >\n",
      "<Epoch :  0 ><Step :  108 ><loss :  87.3749767549 >\n",
      "<Epoch :  0 ><Step :  109 ><loss :  86.0746488976 >\n",
      "<Epoch :  0 ><Step :  110 ><loss :  85.4513836404 >\n",
      "<Epoch :  0 ><Step :  111 ><loss :  86.4479299724 >\n",
      "<Epoch :  0 ><Step :  112 ><loss :  86.1025772418 >\n",
      "<Epoch :  0 ><Step :  113 ><loss :  83.540866464 >\n",
      "<Epoch :  0 ><Step :  114 ><loss :  82.3964583284 >\n",
      "<Epoch :  0 ><Step :  115 ><loss :  90.7459738731 >\n",
      "<Epoch :  0 ><Step :  116 ><loss :  83.2747915791 >\n",
      "<Epoch :  0 ><Step :  117 ><loss :  85.0605843063 >\n",
      "<Epoch :  0 ><Step :  118 ><loss :  82.765695641 >\n",
      "<Epoch :  0 ><Step :  119 ><loss :  84.1536116349 >\n",
      "<Epoch :  0 ><Step :  120 ><loss :  81.0887699267 >\n",
      "<Epoch :  "
     ]
    }
   ],
   "source": [
    "model.fit(training_data,training_data_class)\n",
    "#model.buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1016"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(validating_data)-np.count_nonzero(model.predict(validating_data)-validating_data_class))/5000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_data=sk.preprocessing.normalize(test.iloc[:,1:785].copy().as_matrix())\n",
    "#testing_data=clf.transform(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_results=model.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp=[i for i in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_result=np.array(zip(tmp,test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    4]\n",
      " [   1    4]\n",
      " [   2    2]\n",
      " ..., \n",
      " [9997    3]\n",
      " [9998    7]\n",
      " [9999    6]]\n"
     ]
    }
   ],
   "source": [
    "print csv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('./res.csv',csv_result,delimiter=',',fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
